<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>原生Transformer架构简介 | BoringLeric&#39;s Blog</title>
<meta name="keywords" content="Transformers">
<meta name="description" content="Transformer结构的分别介绍，Encoder和Decoder的拆解。">
<meta name="author" content="BoringLeric">
<link rel="canonical" href="https://boringleric.github.io/posts/tech/vanilla_transformers/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://boringleric.github.io/img/logo.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://boringleric.github.io/img/logo.ico">
<link rel="icon" type="image/png" sizes="32x32" href="https://boringleric.github.io/img/logo.ico">
<link rel="apple-touch-icon" href="https://boringleric.github.io/img/logo.ico">
<link rel="mask-icon" href="https://boringleric.github.io/img/logo.ico">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="https://cdn.staticfile.org/lxgw-wenkai-screen-webfont/1.7.0/style.css" />


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js" integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
<meta property="og:title" content="原生Transformer架构简介" />
<meta property="og:description" content="Transformer结构的分别介绍，Encoder和Decoder的拆解。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://boringleric.github.io/posts/tech/vanilla_transformers/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-01T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-03-01T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="原生Transformer架构简介"/>
<meta name="twitter:description" content="Transformer结构的分别介绍，Encoder和Decoder的拆解。"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "https://boringleric.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Techs",
      "item": "https://boringleric.github.io/posts/tech/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "原生Transformer架构简介",
      "item": "https://boringleric.github.io/posts/tech/vanilla_transformers/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "原生Transformer架构简介",
  "name": "原生Transformer架构简介",
  "description": "Transformer结构的分别介绍，Encoder和Decoder的拆解。",
  "keywords": [
    "Transformers"
  ],
  "articleBody": "使用self-attention的经典模型Transformer，和基于其Encoder结构的预训练模型Bert，为NLP领域开创了新时代。\nTransformer Transformer是在Attention is All You Need提出的的模型框架，用于解决序列转写任务。这里借用经典的图做一下解释：\nTransformer遵循Encoder-Decoder结构，上图左侧Inputs部分为Encoder，右侧Outputs部分为Decoder，两者均为多个Transformer Block堆叠而成，但Encoder的Transformer与Decoder的Transformer是有略微区别的：\nEncoder端使用的attention不需要masked，而Decoder需要Masked； Decoder中多了一层Encoder-Decoder attention，其中的Query是self-attention 计算出的上一时间i处的编码值，Key和Value都是Encoder的输出。 Embedding Embedding是transformer的基础，文本需要转换为embedding，才可送入Encoder做后续运算。Transformer的Embedding包括两个部分，一个是Word embedding，另一个是Positional embedding。\nWord Embedding 与传统Word Embedding技术一致，通过查表方式，根据token的id将token转为embedding。\nPositional Embedding 由于Attention采用的并行计算，导致token的计算逻辑，不具备任何的位置信息，次序打乱之后也不会影响attention的计算结果，所以必须引入位置向量，确保位置信息不会被丢弃。在Transformers中，位置信息使用的绝对位置，构造位置编码的公式如下所示： $$ \\left{ {\\begin{array}{*{20}{c}} {P{E_{2i}}(p) = \\sin (p/{{10000}^{2i/{d_{pos}}}})}\\ {P{E_{2i + 1}}(p) = \\cos (p/{{10000}^{2i/{d_{pos}}}})} \\end{array}} \\right. $$\n对不同维度使用不同频率的正/余弦公式进而生成不同位置的高维位置编码，利用正余弦函数实现相对位置信息的表示。\n为什么奇偶维度之间需要作出区分，分别使用 sin 和 cos 呢？奇偶区分可以通过全连接层帮助重排坐标，所以可以直接简单地分为两段(前 256 维使用 sin，后 256 维使用 cos)。\n组合方法 词向量和位置向量二者直接对应相加，得到最终的向量嵌入。\n这样操作是否会影响到原始的输入特征呢？根据johnson–lindenstrauss 定理：通俗的理解是两个高维的向量总是近似正交的，可以认为无论是add还是concat，两个向量耦合性小，丢失的信息量少，因为维度足够高，所以即使有部分信息被正余弦信号遮蔽，仍然有足够的特征可以用来辨识。\nEncoder 从Encoder段开始介绍，由上图可知，一个Encoder块主要由两部分组成：多头注意力机制(Multi-Head Attention)和前馈神经网络(Feed Forward)，二者都通过残差结构连接。\n多头注意力机制 多头注意力模块结构如下图所示：\n这里可以看出整个模块是由h个Scaled Dot-Product Attention模块堆叠而成的，可以类比CNN中同时使用多个卷积核的作用，有助于捕捉到更丰富的特征信息。主要步骤如下：\n向量生成：将token的embedding经过线性变换，生成Q、K、V三个向量； Attention切分：将Attention按照指定的head数量切分，如512维度分成8个head，每个head维度为64； Self Attention计算：对每个head进行Scaled Self Attention计算； 连接：对于h个Scaled Self Attention的输出，进行简单的拼接，并通过与一个线性映射矩阵${W_o}$与其相乘（目的是对输出矩阵进行压缩），从而得到整个Multi-Head Attention的输出。 下图是一个简单的multi-head attention计算过程：\n输入句子 Thinking Machines; 将原句Tokenizer后转成Word Embedding X，假设维度512； 假设头的个数为8个，将Embedding X切分为8份，每份维度64，并将其分别赋予key、query、value，每个都乘以对应权重矩阵$w_i^k$，$w_i^q$，$w_i^v$，得到输入向量$K_i$，$Q_i$，$V_i$； 8个头分别计算Attention矩阵，${Z_i} = soft\\max (\\frac{{{Q_i}K_i^T}}{\\sqrt {{d_k}}}){V_i}$； concat拼接融合8个头的结果${Z_i}$，点乘权重$W_o$得到最终输出Z。 在Multi-Head Attention中，最关键的部分就是Self-Attention部分了，也就是上图所说的Scaled Dot-Product Attention，计算过程可以用以下的矩阵形式进行并行计算: $$ Attention(Q,K,V) = softmax (\\frac{{Q{K^T}}}{{\\sqrt {{d_k}} }})V $$ 其中，Q, V, K分别表示输入句子的Queries, Keys, Values矩阵，矩阵的每一行为每一个词对应的向量Query, Key, Value向量，$d_k$表示向量长度。也正是因此，Transformer具有RNN所缺失的并行计算能力。\n依旧以Thinking Machines为例，下图给出了两词计算self-attention计算的过程：\n首先对每个输入单词向量Embedding X生成三个对应的向量：Query，Key 和 Value。这三个向量相比于向量X要小的多，因为每一个都会被拆分成headnum个独自计算，并且最后这headnum个输出会拼接恢复成原维度向量； 用Queries和Keys的点积计算所有单词相对于当前词(图中为Thinking)的权重得分，该分数决定在编码单词Thinking时其他单词给予的权重贡献度调整； 将Score除以向量维度64的平方根，再对其进行Softmax，将所有单词的分数进行归一化，这样对于每个单词都会获得所有单词对该单词编码的贡献分数，当然当前单词将获得最大分数，但也将会关注其他单词的贡献大小； 对得到的Softmax分数乘以每一个对应的Value向量； 对所得的所有加权向量求和，即得到Self-Attention对于当前词Thinking的输出。 Tips：\nKQV是怎么从token转换而来的？下面这个图可以给出结论，对于每个向量，都会维护一个映射矩阵，并且不同Self-Attention模块之间的权值不共享，将输入的矩阵X与三个映射矩阵相乘，得到三个输入Queries、Keys和Values。要注意这里的X可以是初始输入的token(也就是Input Embedding + Positional Embedding)也可以是上一层Transformer的输出。 self-attention为什么要除以$\\sqrt {{d_k}}$？\n首先，点积的目的是为了获得query和key的相似度，后续会接softmax进行计算，当query和key之间的方差过大时，softmax计算结果会出现非0即1的情况，会向很小的梯度的方向靠拢，训练效果不佳。而除以$\\sqrt {{d_k}}$缩放了矩阵乘积范围，可以获得更加平滑的softmax结果，训练效果更好；\n假设Q和K是标准正态分布，均值为0，方差为1。它们的矩阵乘积将有均值为0，方差为 ${d_k}$ 。而$\\sqrt {{d_k}}$用于缩放时，Q和K的矩阵乘积的均值为0，方差为1，可以获得一个更平缓的softmax。\n$$ q \\cdot k = \\sum\\nolimits_{i = 1}^{{d_k}} {{q_i}{k_i}} $$\n为什么一定要定义kqv三个元素？\n注意力机制目的是要学习到一个权值，自注意力机制可以通过权重矩阵学习词与词之间的关系，但是如果只定义一个元素，无法分别表示自身与其他元素，其他元素与自身的权重，所以至少需要kq两个元素表示；而找到了权重之后，为了给原值加权，需要另外定义一层可以学习的参数，增强网络的学习能力。\n另外，如果只定义一个元素，那么很显然每个元素对于自己的注意力永远是最大的，挤压了其他的注意力得分，参数太少也影响模型容量。\n有解释是参考了Memory Network，用输入的query检索k-v memories，得到相似memory的key，计算相关性后对value进行加权求和。\n还有解释是对embedding矩阵起到降维，降低运算量的目的。\n前馈神经网络 Transformers的前馈网络层很简单，由两个全连接层构成，第一层的激活函数为 ReLu，第二层不使用激活函数。在Bert代码中，Intermediate Layer作为FFN的第一层全连接层，其维度是（hidden_size，intermediate_size），且intermediate_size比hidden_size大很多，Output Layer作为第二层，不使用激活函数。\n为什么要加入一层前馈神经网络呢？可以看出，多头Attention进行的主要是矩阵乘法，也就是说进行的全都是线性变换，也就是对权重的重新分配，而线性变换的学习能力不如非线性，尽管Attention机制学习到了每个词汇的表达，但是表达能力不强，需要通过激活函数强化表达能力（提供非线性映射，增强数据大的部分，抑制数据小的部分）。\n在论文中对这一部分的解释是，可以将其视作两个具有$1\\times1$卷积核的卷积神经网络，$1\\times1$卷积核可以让每个位置进行独立运算，不需要关注周边信息，而中间扩增的结构类似于Resnet的bottleneck结构，减少计算量，增加网络的表示能力。\nLayer Normalization 在每个Block中，无论是MHA还是FFN都添加了层间归一化模块，随着网络深度的增加，保证数据特征分布的稳定性，加入正则化，使用更大的学习率，加速模型的收敛速度。同时，正则化也有一定的抗过拟合作用，使训练过程更加平稳。\n为什么Transformer使用LayerNorm ，而不使用BatchNorm？BN的特点是强行拉平数据之间的分布，使得模型收敛速度更快，并且起到了正则化的作用，使模型效果更佳。但是，BatchNorm对Batch Size大小很敏感，并且在LSTM网络上效果极差。LayerNorm是横向归一化，不受Batch Size大小的影响，并且可以很好地应用在时序数据中，而且不需要额外的储存空间。《Rethinking Batch Normalization in Transformers》一文对比了LayerNorm和BatchNorm对于Transformer的作用，并且提出了一种新的归一化方式。\n残差模块(Residual Block) 残差模块是借鉴自CNN的思想，可以解决梯度消失和网络退化问题，构造更深的网络。 首先，残差模块能让训练变得更加简单，如果输入值和输出值的差值过小，那么可能梯度会过小，导致出现梯度小时的情况，残差网络的好处在于当残差为0时，改成神经元只是对前层进行一次线性堆叠，使得网络梯度不容易消失，性能不会下降。 其次，随着网络层数的增加，网络会发生退化现象：随着网络层数的增加训练集loss逐渐下降，然后趋于饱和，如果再增加网络深度的话，训练集loss反而会增大，注意这并不是过拟合，因为在过拟合中训练loss是一直减小的。 加入残差网络后，在前向传播时，输入信号可以从任意低层直接传播到高层。由于包含了一个天然的恒等映射，一定程度上可以解决网络退化问题。\nDecoder Transformer的Decoder端主要由三个模块构成，分别是Masked Multi-head attention、Encoder-decoder attention和Feed Forward Layer，除FFL外，另外两个Attention的结构与Encoder端的Attention结构也是一致的，只是在训练和预测阶段操作稍有不同。整体流程图如下所示：\nMasked Multi-head attention 这一个Attention采用了Mask操作，因为在解码过程中，需要顺序进行，得到$i$个词之后，才可以进行$i+1$个词的预测，通过Mask操作可以防止第$i$个词看到后续词的信息。\n需要注意，在Training过程中，需要Mask操作，而在Prediction过程中，无需Mask操作。\nDecoder训练过程中可以使用Teacher forcing和并行化训练，Mask操作如下所示，一般做法是将需要被mask的向量设置为负无穷，那么经过点积和softmax之后其结果趋近于0，对最终结果也几乎没有任何影响，只对QK操作有影响：\n为什么在训练阶段可以并行化训练，因为已经知道了正确的译文，因此可以同时模拟已知前$i$ 个译文token情况下，训练decoder去预测第 $i+1$ 个译文token。\nEncoder-decoder attention 这个部分的Attention的目的是建立Encoder和Decoder之间的关系，主要区别在与Attention的K，V矩阵不是来自上一个Decoder Block的输出，而是来自Encoder端的输出，但是Q矩阵还是来自于上一个Decoder Block的输出。\n输出端 在最后一层Decoder Block后，接Linear将输出转为vocab_size长度，后接softmax算出最大概率logit输出。\nBert 结构 Bert是一种语言模型，使用的是Transformers的Encoder模块，并在embedding层添加了segment embeddings做NSP任务。\n学习任务 MLM任务，主要是预测掩码元素，在原文中15%的token被掩掉，其中80%的token使用[mask]替换，10%的token使用随机token替换，10%的token保持不变。mask太少会导致学习不充分，增加训练时长；mask太多会导致使一段文本中丢失太多的语义信息。（跟CBOW相似，15%相当于7个token预测一个token，刚刚好。） NSP任务：预测下一句话，是否是前一句话的下一句话。这个任务主要使用CLS的Embedding，它能否表示sentense embedding呢？从任务可见，pretrain的cls embedding很大程度上就是在编码NSP任务所需的高阶特征，也就是描述两段文本是否构成上下文关系，所以说，这个特征既然不是描述sentence语义的，直接用到sentence embedding上效果不好，但是finetuning之后可以获得不错的效果。 BERT中的token mask方法对理解文本语义任务极有帮助，但这种方式具有硬伤： 预训练时输入句子带mask，而fine-tune不带，导致上下游任务不匹配问题。 transformer的encoder的all attention结构，需要句子all token参与attention，这注定与自回归的NLG任务无法配套使用。 分词问题 词表的构建选用基于BPE算法的WordPiece算法，将通过频率合并字符的频率变成了通过语言模型的似然进行合并，其思想是选择能够提升语言模型概率最大的相邻子词加入词表。\nBPE与Wordpiece都是首先初始化一个小词表，再根据一定准则将不同的子词合并。\nBPE与Wordpiece的最大区别在于，如何选择两个子词进行合并：BPE选择频数最高的相邻子词合并，而WordPiece选择能够提升语言模型概率最大的相邻子词加入词表。\nBasicTokenizer： 转unicode -\u003e 去除空字符、替换字符、控制字符和空白字符等奇怪字符 -\u003e 中文分词 -\u003e 空格分词 -\u003e 小写、去掉变音符号、标点分词；\nWordpieceTokenizer：大致分词思路是贪婪最长优先匹配算法，按照从左到右的顺序，将一个词拆分成多个子词，每个子词尽可能长。\n对中文：BERT 采取的是将每一个汉字都切开的策略。\nWhole-word-mask策略：WordPiece构建的词表，会有很多子词，掩码时会导致信息不全。引入全词掩码，若一个word被切开，则全部切分的子词都要被掩掉。\n可能的问题：\n词汇量不足：对于英文词汇，专有名词被拆分； 拼写错误/缩写问题：拼写错误/缩写可能会导致模型性能大幅下降，subword导致切分与原文完全不一致； 前缀问题：WordPiece 旨在处理后缀和简单的复合词，没有前缀的概念。 解决方法：\n超大规模预训练数据和模型结构； 纠错，预处理删除无意义字符串，缩写扩充； 分解复合词，将前缀视为复合词并将它们分开。 Quiz Bert模型的做句向量的缺陷：直接使用Bert做句向量的输出，会发现所有的句向量两两相似度都很高。因为对于句子来说，大多数的句子都是使用常见的词组成的，Bert的词向量空间是非凸的，大量的常见的词向量都是在0点附近，从而计算出的句子向量，都很相似。 如何解决Bert句向量的缺陷：使用双塔模型，将两个句子传入两个参数共享的Bert模型，将两个句向量做拼接，进行有监督的学习，从而调整Bert参数，此方法叫sentencebert；使用无监督或者有监督的对比学习，将同一个句子传入相同的bert(dropout = 0.3)得到标签为正例的一个句子对。通过这种方式来做Bert的微调整，得到SimCSE模型。 参考 The Illustrated Transformer Transformers Explained Visually 深入理解Transformer模型 ",
  "wordCount" : "6761",
  "inLanguage": "zh",
  "datePublished": "2023-03-01T00:00:00Z",
  "dateModified": "2023-03-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "BoringLeric"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://boringleric.github.io/posts/tech/vanilla_transformers/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "BoringLeric's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://boringleric.github.io/img/logo.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://boringleric.github.io/" accesskey="h" title="BoringLeric&#39;s Blog (Alt + H)">
                <img src="https://boringleric.github.io/img/logo.ico" alt="" aria-label="logo"
                    height="35">BoringLeric&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://boringleric.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://boringleric.github.io/search" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://boringleric.github.io/posts" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://boringleric.github.io/tags" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://boringleric.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://boringleric.github.io/about" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://boringleric.github.io/">主页</a>&nbsp;»&nbsp;<a href="https://boringleric.github.io/posts/">Blogs</a>&nbsp;»&nbsp;<a href="https://boringleric.github.io/posts/tech/">Techs</a></div>
            <h1 class="post-title">
                原生Transformer架构简介
            </h1>
            <div class="post-meta"><span title='2023-03-01 00:00:00 +0000 UTC'>2023-03-01</span>&nbsp;·&nbsp;14 分钟&nbsp;·&nbsp;BoringLeric

</div>
        </header><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#transformer" aria-label="Transformer">Transformer</a><ul>
                        
                <li>
                    <a href="#embedding" aria-label="Embedding">Embedding</a><ul>
                        
                <li>
                    <a href="#word-embedding" aria-label="Word Embedding">Word Embedding</a></li>
                <li>
                    <a href="#positional-embedding" aria-label="Positional Embedding">Positional Embedding</a></li>
                <li>
                    <a href="#%e7%bb%84%e5%90%88%e6%96%b9%e6%b3%95" aria-label="组合方法">组合方法</a></li></ul>
                </li>
                <li>
                    <a href="#encoder" aria-label="Encoder">Encoder</a><ul>
                        
                <li>
                    <a href="#%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6" aria-label="多头注意力机制">多头注意力机制</a></li>
                <li>
                    <a href="#%e5%89%8d%e9%a6%88%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c" aria-label="前馈神经网络">前馈神经网络</a></li>
                <li>
                    <a href="#layer-normalization" aria-label="Layer Normalization">Layer Normalization</a></li>
                <li>
                    <a href="#%e6%ae%8b%e5%b7%ae%e6%a8%a1%e5%9d%97residual-block" aria-label="残差模块(Residual Block)">残差模块(Residual Block)</a></li></ul>
                </li>
                <li>
                    <a href="#decoder" aria-label="Decoder">Decoder</a><ul>
                        
                <li>
                    <a href="#masked-multi-head-attention" aria-label="Masked Multi-head attention">Masked Multi-head attention</a></li>
                <li>
                    <a href="#encoder-decoder-attention" aria-label="Encoder-decoder attention">Encoder-decoder attention</a></li>
                <li>
                    <a href="#%e8%be%93%e5%87%ba%e7%ab%af" aria-label="输出端">输出端</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#bert" aria-label="Bert">Bert</a><ul>
                        <ul>
                        
                <li>
                    <a href="#%e7%bb%93%e6%9e%84" aria-label="结构">结构</a></li>
                <li>
                    <a href="#%e5%ad%a6%e4%b9%a0%e4%bb%bb%e5%8a%a1" aria-label="学习任务">学习任务</a></li>
                <li>
                    <a href="#%e5%88%86%e8%af%8d%e9%97%ae%e9%a2%98" aria-label="分词问题">分词问题</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#quiz" aria-label="Quiz">Quiz</a></li>
                <li>
                    <a href="#%e5%8f%82%e8%80%83" aria-label="参考">参考</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        if (elements) {
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                    (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                    return element;
                }
            }) || activeElement

            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                if (element === activeElement){
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
                } else {
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
                }
            })
        }
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><p>使用self-attention的经典模型Transformer，和基于其Encoder结构的预训练模型Bert，为NLP领域开创了新时代。</p>
<h4 id="transformer">Transformer<a hidden class="anchor" aria-hidden="true" href="#transformer">#</a></h4>
<p>Transformer是在Attention is All You Need提出的的模型框架，用于解决序列转写任务。这里借用经典的图做一下解释：</p>
<p><img loading="lazy" src="/vanilla_transformers/1816627-20190930142150498-984956485.png" alt="img"  />
</p>
<p>Transformer遵循Encoder-Decoder结构，上图左侧Inputs部分为Encoder，右侧Outputs部分为Decoder，两者均为多个Transformer Block堆叠而成，但Encoder的Transformer与Decoder的Transformer是有略微区别的：</p>
<ul>
<li>Encoder端使用的attention不需要masked，而Decoder需要Masked；</li>
<li>Decoder中多了一层Encoder-Decoder attention，其中的Query是self-attention 计算出的上一时间i处的编码值，Key和Value都是Encoder的输出。</li>
</ul>
<h5 id="embedding">Embedding<a hidden class="anchor" aria-hidden="true" href="#embedding">#</a></h5>
<p>Embedding是transformer的基础，文本需要转换为embedding，才可送入Encoder做后续运算。Transformer的Embedding包括两个部分，一个是Word embedding，另一个是Positional embedding。</p>
<h6 id="word-embedding">Word Embedding<a hidden class="anchor" aria-hidden="true" href="#word-embedding">#</a></h6>
<p>与传统Word Embedding技术一致，通过查表方式，根据token的id将token转为embedding。</p>
<h6 id="positional-embedding">Positional Embedding<a hidden class="anchor" aria-hidden="true" href="#positional-embedding">#</a></h6>
<p>由于Attention采用的并行计算，导致token的计算逻辑，不具备任何的位置信息，次序打乱之后也不会影响attention的计算结果，所以必须引入位置向量，确保位置信息不会被丢弃。在Transformers中，位置信息使用的绝对位置，构造位置编码的公式如下所示：
$$
\left{ {\begin{array}{*{20}{c}}
{P{E_{2i}}(p) = \sin (p/{{10000}^{2i/{d_{pos}}}})}\
{P{E_{2i + 1}}(p) = \cos (p/{{10000}^{2i/{d_{pos}}}})}
\end{array}} \right.
$$</p>
<p>对不同维度使用不同频率的正/余弦公式进而生成不同位置的高维位置编码，利用正余弦函数实现相对位置信息的表示。</p>
<p>为什么奇偶维度之间需要作出区分，分别使用 sin 和 cos 呢？奇偶区分可以通过全连接层帮助重排坐标，所以可以直接简单地分为两段(前 256 维使用 sin，后 256 维使用 cos)。</p>
<h6 id="组合方法">组合方法<a hidden class="anchor" aria-hidden="true" href="#组合方法">#</a></h6>
<p>词向量和位置向量二者直接对应相加，得到最终的向量嵌入。</p>
<p>这样操作是否会影响到原始的输入特征呢？根据johnson–lindenstrauss 定理：通俗的理解是两个高维的向量总是近似正交的，可以认为无论是add还是concat，两个向量耦合性小，丢失的信息量少，因为维度足够高，所以即使有部分信息被正余弦信号遮蔽，仍然有足够的特征可以用来辨识。</p>
<h5 id="encoder">Encoder<a hidden class="anchor" aria-hidden="true" href="#encoder">#</a></h5>
<p>从Encoder段开始介绍，由上图可知，一个Encoder块主要由两部分组成：多头注意力机制(Multi-Head Attention)和前馈神经网络(Feed Forward)，二者都通过残差结构连接。</p>
<h6 id="多头注意力机制">多头注意力机制<a hidden class="anchor" aria-hidden="true" href="#多头注意力机制">#</a></h6>
<p>多头注意力模块结构如下图所示：</p>
<p><img loading="lazy" src="/vanilla_transformers/mha_img_original.png" alt="img"  />
这里可以看出整个模块是由h个Scaled Dot-Product Attention模块堆叠而成的，可以类比CNN中同时使用多个卷积核的作用，有助于捕捉到更丰富的特征信息。主要步骤如下：</p>
<ol>
<li>向量生成：将token的embedding经过线性变换，生成Q、K、V三个向量；</li>
<li>Attention切分：将Attention按照指定的head数量切分，如512维度分成8个head，每个head维度为64；</li>
<li>Self Attention计算：对每个head进行Scaled Self Attention计算；</li>
<li>连接：对于h个Scaled Self Attention的输出，进行简单的拼接，并通过与一个线性映射矩阵${W_o}$与其相乘（目的是对输出矩阵进行压缩），从而得到整个Multi-Head Attention的输出。</li>
</ol>
<p>下图是一个简单的multi-head attention计算过程：</p>
<ol>
<li>输入句子 Thinking Machines;</li>
<li>将原句Tokenizer后转成Word Embedding X，假设维度512；</li>
<li>假设头的个数为8个，将Embedding X切分为8份，每份维度64，并将其分别赋予key、query、value，每个都乘以对应权重矩阵$w_i^k$，$w_i^q$，$w_i^v$，得到输入向量$K_i$，$Q_i$，$V_i$；</li>
<li>8个头分别计算Attention矩阵，${Z_i} = soft\max (\frac{{{Q_i}K_i^T}}{\sqrt {{d_k}}}){V_i}$；</li>
<li>concat拼接融合8个头的结果${Z_i}$，点乘权重$W_o$得到最终输出Z。</li>
</ol>
<p><img loading="lazy" src="/vanilla_transformers/transformer_multi-headed_self-attention-recap.png" alt="img"  />
</p>
<p>在Multi-Head Attention中，最关键的部分就是Self-Attention部分了，也就是上图所说的Scaled Dot-Product Attention，计算过程可以用以下的矩阵形式进行并行计算:
$$
Attention(Q,K,V) = softmax (\frac{{Q{K^T}}}{{\sqrt {{d_k}} }})V
$$
其中，Q, V, K分别表示输入句子的Queries, Keys, Values矩阵，矩阵的每一行为每一个词对应的向量Query, Key, Value向量，$d_k$表示向量长度。也正是因此，Transformer具有RNN所缺失的并行计算能力。</p>
<p>依旧以Thinking Machines为例，下图给出了两词计算self-attention计算的过程：</p>
<ol>
<li>首先对每个输入单词向量Embedding X生成三个对应的向量：Query，Key 和 Value。这三个向量相比于向量X要小的多，因为每一个都会被拆分成headnum个独自计算，并且最后这headnum个输出会拼接恢复成原维度向量；</li>
<li>用Queries和Keys的点积计算所有单词相对于当前词(图中为Thinking)的权重得分，该分数决定在编码单词Thinking时其他单词给予的权重贡献度调整；</li>
<li>将Score除以向量维度64的平方根，再对其进行Softmax，将所有单词的分数进行归一化，这样对于每个单词都会获得所有单词对该单词编码的贡献分数，当然当前单词将获得最大分数，但也将会关注其他单词的贡献大小；</li>
<li>对得到的Softmax分数乘以每一个对应的Value向量；</li>
<li>对所得的所有加权向量求和，即得到Self-Attention对于当前词Thinking的输出。</li>
</ol>
<p><img loading="lazy" src="/vanilla_transformers/1816627-20190930142222180-936702842.png" alt="img"  />
</p>
<p>Tips：</p>
<ol>
<li>KQV是怎么从token转换而来的？下面这个图可以给出结论，对于每个向量，都会维护一个映射矩阵，并且不同Self-Attention模块之间的权值不共享，将输入的矩阵X与三个映射矩阵相乘，得到三个输入Queries、Keys和Values。要注意这里的X可以是初始输入的token(也就是Input Embedding + Positional Embedding)也可以是上一层Transformer的输出。</li>
</ol>
<p><img loading="lazy" src="/vanilla_transformers/self-attention-matrix-calculation.png" alt="img"  />
</p>
<ol start="2">
<li>
<p>self-attention为什么要除以$\sqrt {{d_k}}$？</p>
<ul>
<li>
<p>首先，点积的目的是为了获得query和key的相似度，后续会接softmax进行计算，当query和key之间的方差过大时，softmax计算结果会出现非0即1的情况，会向很小的梯度的方向靠拢，训练效果不佳。而除以$\sqrt {{d_k}}$缩放了矩阵乘积范围，可以获得更加平滑的softmax结果，训练效果更好；</p>
</li>
<li>
<p>假设Q和K是标准正态分布，均值为0，方差为1。它们的矩阵乘积将有均值为0，方差为 ${d_k}$ 。而$\sqrt {{d_k}}$用于缩放时，Q和K的矩阵乘积的均值为0，方差为1，可以获得一个更平缓的softmax。</p>
</li>
</ul>
</li>
</ol>
<p>$$
q \cdot k = \sum\nolimits_{i = 1}^{{d_k}} {{q_i}{k_i}}
$$</p>
<ol start="3">
<li>
<p>为什么一定要定义kqv三个元素？</p>
<p>注意力机制目的是要学习到一个权值，自注意力机制可以通过权重矩阵学习词与词之间的关系，但是如果只定义一个元素，无法分别表示自身与其他元素，其他元素与自身的权重，所以至少需要kq两个元素表示；而找到了权重之后，为了给原值加权，需要另外定义一层可以学习的参数，增强网络的学习能力。</p>
<p>另外，如果只定义一个元素，那么很显然每个元素对于自己的注意力永远是最大的，挤压了其他的注意力得分，参数太少也影响模型容量。</p>
<p>有解释是参考了Memory Network，用输入的query检索k-v memories，得到相似memory的key，计算相关性后对value进行加权求和。</p>
<p>还有解释是对embedding矩阵起到降维，降低运算量的目的。</p>
</li>
</ol>
<h6 id="前馈神经网络">前馈神经网络<a hidden class="anchor" aria-hidden="true" href="#前馈神经网络">#</a></h6>
<p>Transformers的前馈网络层很简单，由两个全连接层构成，第一层的激活函数为 ReLu，第二层不使用激活函数。在Bert代码中，Intermediate Layer作为FFN的第一层全连接层，其维度是（hidden_size，intermediate_size），且intermediate_size比hidden_size大很多，Output Layer作为第二层，不使用激活函数。</p>
<p>为什么要加入一层前馈神经网络呢？可以看出，多头Attention进行的主要是矩阵乘法，也就是说进行的全都是线性变换，也就是对权重的重新分配，而线性变换的学习能力不如非线性，尽管Attention机制学习到了每个词汇的表达，但是表达能力不强，需要通过激活函数强化表达能力（提供非线性映射，增强数据大的部分，抑制数据小的部分）。</p>
<p>在论文中对这一部分的解释是，可以将其视作两个具有$1\times1$卷积核的卷积神经网络，$1\times1$卷积核可以让每个位置进行独立运算，不需要关注周边信息，而中间扩增的结构类似于Resnet的bottleneck结构，减少计算量，增加网络的表示能力。</p>
<h6 id="layer-normalization">Layer Normalization<a hidden class="anchor" aria-hidden="true" href="#layer-normalization">#</a></h6>
<p>在每个Block中，无论是MHA还是FFN都添加了层间归一化模块，随着网络深度的增加，保证数据特征分布的稳定性，加入正则化，使用更大的学习率，加速模型的收敛速度。同时，正则化也有一定的抗过拟合作用，使训练过程更加平稳。</p>
<p>为什么Transformer使用LayerNorm ，而不使用BatchNorm？BN的特点是强行拉平数据之间的分布，使得模型收敛速度更快，并且起到了正则化的作用，使模型效果更佳。但是，BatchNorm对Batch Size大小很敏感，并且在LSTM网络上效果极差。LayerNorm是横向归一化，不受Batch Size大小的影响，并且可以很好地应用在时序数据中，而且不需要额外的储存空间。《Rethinking Batch Normalization in Transformers》一文对比了LayerNorm和BatchNorm对于Transformer的作用，并且提出了一种新的归一化方式。</p>
<h6 id="残差模块residual-block">残差模块(Residual Block)<a hidden class="anchor" aria-hidden="true" href="#残差模块residual-block">#</a></h6>
<p>残差模块是借鉴自CNN的思想，可以解决梯度消失和网络退化问题，构造更深的网络。
首先，残差模块能让训练变得更加简单，如果输入值和输出值的差值过小，那么可能梯度会过小，导致出现梯度小时的情况，残差网络的好处在于当残差为0时，改成神经元只是对前层进行一次线性堆叠，使得网络梯度不容易消失，性能不会下降。
其次，随着网络层数的增加，网络会发生退化现象：随着网络层数的增加训练集loss逐渐下降，然后趋于饱和，如果再增加网络深度的话，训练集loss反而会增大，注意这并不是过拟合，因为在过拟合中训练loss是一直减小的。
加入残差网络后，在前向传播时，输入信号可以从任意低层直接传播到高层。由于包含了一个天然的恒等映射，一定程度上可以解决网络退化问题。</p>
<h5 id="decoder">Decoder<a hidden class="anchor" aria-hidden="true" href="#decoder">#</a></h5>
<p>Transformer的Decoder端主要由三个模块构成，分别是Masked Multi-head attention、Encoder-decoder attention和Feed Forward Layer，除FFL外，另外两个Attention的结构与Encoder端的Attention结构也是一致的，只是在训练和预测阶段操作稍有不同。整体流程图如下所示：</p>
<p><img loading="lazy" src="/vanilla_transformers/transformer_resideual_layer_norm_3.png" alt="img"  />
</p>
<h6 id="masked-multi-head-attention">Masked Multi-head attention<a hidden class="anchor" aria-hidden="true" href="#masked-multi-head-attention">#</a></h6>
<p>这一个Attention采用了Mask操作，因为在解码过程中，需要顺序进行，得到$i$个词之后，才可以进行$i+1$个词的预测，通过Mask操作可以防止第$i$个词看到后续词的信息。</p>
<p>需要注意，在Training过程中，需要Mask操作，而在Prediction过程中，无需Mask操作。</p>
<p>Decoder训练过程中可以使用Teacher forcing和并行化训练，Mask操作如下所示，一般做法是将需要被mask的向量设置为负无穷，那么经过点积和softmax之后其结果趋近于0，对最终结果也几乎没有任何影响，只对QK操作有影响：</p>
<p><img loading="lazy" src="/vanilla_transformers/transformer-decoder-mask-matmul.png" alt="img"  />
</p>
<p>为什么在训练阶段可以并行化训练，因为已经知道了正确的译文，因此可以同时模拟已知前$i$ 个译文token情况下，训练decoder去预测第 $i+1$ 个译文token。</p>
<h6 id="encoder-decoder-attention">Encoder-decoder attention<a hidden class="anchor" aria-hidden="true" href="#encoder-decoder-attention">#</a></h6>
<p>这个部分的Attention的目的是建立Encoder和Decoder之间的关系，主要区别在与Attention的K，V矩阵不是来自上一个Decoder Block的输出，而是来自Encoder端的输出，但是Q矩阵还是来自于上一个Decoder Block的输出。</p>
<h6 id="输出端">输出端<a hidden class="anchor" aria-hidden="true" href="#输出端">#</a></h6>
<p>在最后一层Decoder Block后，接Linear将输出转为vocab_size长度，后接softmax算出最大概率logit输出。</p>
<h4 id="bert">Bert<a hidden class="anchor" aria-hidden="true" href="#bert">#</a></h4>
<h6 id="结构">结构<a hidden class="anchor" aria-hidden="true" href="#结构">#</a></h6>
<p>Bert是一种语言模型，使用的是Transformers的Encoder模块，并在embedding层添加了segment embeddings做NSP任务。</p>
<h6 id="学习任务">学习任务<a hidden class="anchor" aria-hidden="true" href="#学习任务">#</a></h6>
<ol>
<li>MLM任务，主要是预测掩码元素，在原文中15%的token被掩掉，其中80%的token使用[mask]替换，10%的token使用随机token替换，10%的token保持不变。mask太少会导致学习不充分，增加训练时长；mask太多会导致使一段文本中丢失太多的语义信息。（跟CBOW相似，15%相当于7个token预测一个token，刚刚好。）</li>
<li>NSP任务：预测下一句话，是否是前一句话的下一句话。这个任务主要使用CLS的Embedding，它能否表示sentense embedding呢？从任务可见，pretrain的cls embedding很大程度上就是在编码NSP任务所需的高阶特征，也就是描述两段文本是否构成上下文关系，所以说，这个特征既然不是描述sentence语义的，直接用到sentence embedding上效果不好，但是finetuning之后可以获得不错的效果。</li>
<li>BERT中的token mask方法对理解文本语义任务极有帮助，但这种方式具有硬伤：
<ol>
<li>预训练时输入句子带mask，而fine-tune不带，导致上下游任务不匹配问题。</li>
<li>transformer的encoder的all attention结构，需要句子all token参与attention，这注定与自回归的NLG任务无法配套使用。</li>
</ol>
</li>
</ol>
<h6 id="分词问题">分词问题<a hidden class="anchor" aria-hidden="true" href="#分词问题">#</a></h6>
<p>词表的构建选用基于BPE算法的WordPiece算法，将通过频率合并字符的频率变成了通过语言模型的似然进行合并，其思想是选择能够提升语言模型概率最大的相邻子词加入词表。</p>
<p>BPE与Wordpiece都是首先初始化一个小词表，再根据一定准则将不同的子词合并。</p>
<p>BPE与Wordpiece的最大区别在于，如何选择两个子词进行合并：BPE选择频数最高的相邻子词合并，而WordPiece选择能够提升语言模型概率最大的相邻子词加入词表。</p>
<p>BasicTokenizer： 转unicode -&gt; 去除空字符、替换字符、控制字符和空白字符等奇怪字符 -&gt; 中文分词 -&gt; 空格分词 -&gt; 小写、去掉变音符号、标点分词；</p>
<p>WordpieceTokenizer：大致分词思路是贪婪最长优先匹配算法，按照从左到右的顺序，将一个词拆分成多个子词，每个子词尽可能长。</p>
<p>对中文：BERT 采取的是将每一个汉字都切开的策略。</p>
<p>Whole-word-mask策略：WordPiece构建的词表，会有很多子词，掩码时会导致信息不全。引入全词掩码，若一个word被切开，则全部切分的子词都要被掩掉。</p>
<p>可能的问题：</p>
<ul>
<li>词汇量不足：对于英文词汇，专有名词被拆分；</li>
<li>拼写错误/缩写问题：拼写错误/缩写可能会导致模型性能大幅下降，subword导致切分与原文完全不一致；</li>
<li>前缀问题：WordPiece 旨在处理后缀和简单的复合词，没有前缀的概念。</li>
</ul>
<p>解决方法：</p>
<ul>
<li>超大规模预训练数据和模型结构；</li>
<li>纠错，预处理删除无意义字符串，缩写扩充；</li>
<li>分解复合词，将前缀视为复合词并将它们分开。</li>
</ul>
<h4 id="quiz">Quiz<a hidden class="anchor" aria-hidden="true" href="#quiz">#</a></h4>
<ol>
<li>Bert模型的做句向量的缺陷：直接使用Bert做句向量的输出，会发现所有的句向量两两相似度都很高。因为对于句子来说，大多数的句子都是使用常见的词组成的，Bert的词向量空间是非凸的，大量的常见的词向量都是在0点附近，从而计算出的句子向量，都很相似。</li>
<li>如何解决Bert句向量的缺陷：使用双塔模型，将两个句子传入两个参数共享的Bert模型，将两个句向量做拼接，进行有监督的学习，从而调整Bert参数，此方法叫sentencebert；使用无监督或者有监督的对比学习，将同一个句子传入相同的bert(dropout = 0.3)得到标签为正例的一个句子对。通过这种方式来做Bert的微调整，得到SimCSE模型。</li>
</ol>
<h4 id="参考">参考<a hidden class="anchor" aria-hidden="true" href="#参考">#</a></h4>
<ol>
<li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
<li><a href="https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853">Transformers Explained Visually </a></li>
<li><a href="https://lulaoshi.info/machine-learning/attention/transformer-attention">深入理解Transformer模型</a></li>
</ol>


        </div>

    </div>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://boringleric.github.io/">BoringLeric&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = '复制';

        function copyingDone() {
            copybutton.innerHTML = '已复制！';
            setTimeout(() => {
                copybutton.innerHTML = '复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
