[{"content":"在信息爆炸的时代，获取有价值的内容离不开检索算法，传统的检索算法主要是通过计算文本检索词和文档相关性排序获得结果，而BERT等预训练模型横空出世之后，向量检索也成了一种新方法。\n文本检索 传统文本检索方法最出名的是BM25算法，这也是众多检索引擎（Lucene，Elastic Search，Xapian）的核心算法，它们应对查询语句的方式主要是切词等一系列处理方法。\nBM25的公式如下所示： $$ ∑_i^nW(q_i)Score(q_i,D)Score(q_i,Q) $$ 其中：\n$q_i$ 表示查询语句中的第 i 个单词，D 表示查询的文档，Q 表示整个查询语句； $W(q_i)$ 表示单词权重，也就是TF-IDF算法里面的IDF：$ln(1+[(N-df_i+0.5)/(df_i+0.5)])$ ，其中N表示索引全部文档数，$df_i$ 表示包含检索词文档的个数。当这个词在文档中出现频率越高，该词对于最终分数的贡献就越小； $Score(q_i,D)$ 表示单词与文档的相关性：$[(k_1+1)tf_{td}]/[k_1(1-b+b*(L_d/L{ave}))+tf_{td}]$ ，其中 $k_1$ 是非负超参数，来标准化文章的词频范围，默认为1.2；b 是范围0-1的超参数，来决定文档长度包含信息量的范围，默认为0.75； $tf_{td}$ 是单词t在文档d中的词频，可以认为出现次数越多，分数越高；$L_d$ 是文档d的长度，$L{ave}$ 是所有文档的平均长度，可以认为同样的频率下，长文档带来的增益比短文档小得多； $Score(q_i,Q)$ 表示单词与查询语句的相关性，当查询的语句较长的时候这一项比较重要：$[(k_3+1)tf_{tq}]/[k_3+tf_{tq}]$ ，其中 $k_3$ 是非负超参数，来调节query的词频范围，$tf_{tq}$ 是单词t在查询语句q中的词频。 在实际使用中，由于知识库的文档质量问题，很可能导致 IDF 指标无法正确衡量指定词汇的权重，特别是某些重点词在知识库频繁出现，甚至可能和停用词频率接近。为了应对这个问题，可以试图引入自定义词频，或者录入部分外部文本来平衡知识库。\n除此之外，这类算法的关键在于精确匹配能力，所以一词多义问题很难解决，通过自定义停用词表、同义词表可以获得一定程度的泛化性能。而难点在于这是采用真正的“文本”匹配，所以如果存在语义上的一致而文本表达方面不一致的情况，即使使用同义词改写也不会有很好的命中效果；同时因为不考虑语序问题，所以很可能出现语义飘移问题，即命中词一致，顺序不一致导致语义不一致。\n向量检索 向量检索并不像文本检索那样将检索语句拆成一个个词去进行查找匹配，它们的主要手段是通过一些模型获得句子的向量表征，并通过一些向量检索引擎去获取文本库中较为接近的句向量。\n句子嵌入本质上是对文本序列中信息的压缩，而压缩本质上是有损的。这意味着句子嵌入是粒度较低级别的表示形式。模型的瓶颈并不完全在于单个向量的表示能力不足，编码器的能力也会在很大程度上影响模型的泛化能力。\n由此可知，向量检索具有良好的语义相似性，无需100%的命中关键词也可以匹配文档，但下列问题仍需要重视：深度模型的选择、知识库的构建以及检索算法的选择，下面将分别进行阐述。\n模型选择 模型要考量的因素很多：\n输入序列长度：关系到输入限制，超长文本可能截断导致语义不完整； 向量维度问题：关系到向量数据库性能，在文本量较多，且效果差距不大的情况下，高维向量会给向量数据库的检索和部署带来很大压力； 语义表征问题：未经垂直领域 Fine-tune 的模型通常效果不会太理想。 另外，很明显的一个问题，由于目前训练算法导致精确匹配能力不足，针对专有名词的精准召回效果不佳，对于N vs 1，1 vs N 的语义理解情况支持不佳。\n目前最常用的句子表征模型训练方法是对比学习，训练样本由三部分组成：[ query，query+，query- ]。一般采用 infoNCE loss 作为优化目标： $$ L_i = −log(e^{S(z_i,z_i^+)/τ}/∑_{j=0}^K e^{S(z_i,z_j)/τ}) $$ 这个 loss 看起来很像套了 log 的 Softmax。其中，S 函数两个向量的余弦相似度， $τ$ 是控制难样本关注度的温度系数，趋向于 0 时更关注难负样本。\n下面两个是比较典型的通用句子向量和它们采用的训练方法：\nM3E：in-batch负采样对比学习 + Instruction Tuning 指令微调\nBGE：RetroMAE预训练（Encoder+简单结构Decoder） + 负采样和难负样例挖掘 + Instruction Tuning 指令微调\n知识库构建 知识库构建的最大问题是文档的切分，如何保证切分合理，并且维持语义完整性。这个问题需要结合实际业务进行考量：\n对于 FAQ 任务来说，按照 QA 对进行拆分是比较好的，向量检索很容易可以使用 Query-Question 召回，但是当用户输入的问题不一定和 FAQ 的 Question 的提问角度一致时，可能Query-Documentary召回会更好一点，二者之间的协同是一个难题； 对于长文本来说，可以借鉴Langchain的简单粗暴切分法，就是按照 ‘\\n’ 进行段落拆分，超长的话直接切断，这样可能会造成一些关键信息的拆离，针对特殊结构文档可以考虑使用更细节的拆分方式。 向量检索算法 在常用的向量检索引擎中，索引类型分为如下几类：\nFLAT：最基础的索引结构，一定可以找到全局最优解，适用于数据量较小的情况。\nIVF(Inverted File)：倒排快速索引，基于决策树的索引，将数据分为多个子空间存储索引信息，增量更新不可以。在向量中如何使用倒排呢？可以拿出每个聚类中心下的向量ID，每个中心ID后面挂上一堆非中心向量，每次查询向量的时候找到最近的几个中心ID，分别搜索这几个中心下的非中心向量。通过减小搜索范围，提升搜索效率。\nPQ：乘积量化，将一个向量的维度切成x段，每段分别进行检索，每段向量的检索结果取交集后得出最后的TopK。因此速度很快，而且占用内存较小，召回率也相对较高。\nHNSW：图检索方案，把库中的结点随机插入图中，每次插入结点的时候都找图中和被插入结点最近的M个每个结点连边，构建索引非常慢，内存占用较大；\nAnnoy：树检索方案，空间划分过程可以看作聚类数为2的KMeans过程，收敛后在产生的两个聚类中心连线之间建立一条垂线，把数据空间划分为两部分。\n计量类型有如下几种：余弦、内积、L2、Jaccacd。归一化后计算的欧式距离是关于余弦相似的单调函数，可以认为归一化后，余弦相似与欧式距离效果是一致的（欧式距离越小等价于余弦相似度越大）。\n二者对比和融合 向量匹配和BM25的对比试验，相关论文显示：\nBM25精确匹配能力强，但是在应对多义词、OOV、变体词方面存在不足； 向量匹配具有BM25不具备的软匹配能力，但粒度问题难以评估、训练相似性与业务相似性的差距、重复语句处理、OOV问题、长尾数据均衡、精确匹配的任务仍需要研究； 二者对于头部文档相关性的定义有着很大的不同，向量匹配会高估不相关文档，而BM25会低估语义相关文档。 向量模型召回结果的 Rerank 前面提到了两种召回，如何结合这两路不在一个体系的结果是一个问题。如果是大模型场景，可以借助大模型的信息抽取能力，简单采用 TopK 去重做倒数加权的方式。而对于非大模型场景，一般考虑训练一个排序模型来进行相关性打分。而排序模型的训练又是另一个重要领域了，这里暂时先挖坑。\n","permalink":"https://boringleric.github.io/posts/tech/thinking_about_retrieval/","summary":"检索是个挺好玩的东西，BM25 vs 向量，各有优劣。","title":"简单思考检索系统"},{"content":"提前准备 机票：香港航空 HX657/658 免费往返机票，但并不免燃油附加费等，共计818元；\n日本签证：淘宝单次签证300元；\n港澳通行证签注：单次15元；\n高铁票：广州 - 香港，单程215元（如果广州 - 深圳北，深圳北 - 香港会更便宜）；\n电话卡：钱多的话可以直接开通漫游，我这里是先在淘宝购买日本电话卡，7天5G，30元，事实证明根本用不了那么多，日本很多景点都有Wifi，但是需要使用邮件注册一下；如果是第一次去日本，可以考虑注册Wamazing，会提供免费的500M流量sim卡，在机场领取即可，搜一些兑换码最高可以获得1.5G流量，在旅行期间不刷视频的话很够用了；\nVJW：日本入境申报电子系统，推荐预先填报，这样就不需要填写纸质入境单了，填写方法可以网上搜索。\nDay1 早早出发去广州南站，出境使用港澳通行证，入香港使用港澳通行证，香港西九龙高铁站走10分钟左右，到达九龙站，乘坐机场快线约20分钟到达香港国际机场。\n机场快线可以使用八达通，也可以用在网上买的电子版，我们用的kkday购买的香港机场快线车票优惠（电子票），82元单人单程，刷二维码即可直接进站，推荐再在大陆或者香港西九龙站内连接港铁Wifi进行购买，否则在香港是没信号的。\n在机场柜台办理登机牌和行李托运业务，香港航空职员还是很专业的根据大陆护照讲了普通话，由于香港人护照免签日本，所以他们会额外检查大陆游客是否持有日本签证，包括在登机前也会再次进行核查。\n注意，香港机场出境也需要刷和入境一致的证件，也就是可以刷港澳通行证自助出境，不要刷成护照。\n随后就是漫长的飞行过程，航线会经过台湾上空，天气好的话可以看到台湾山脉。12月份算是冲绳的旅游淡季，所以入境处人数并不多，给入境管刷VJW，展示电子签就可以了，接下来领行李，过海关，都有足够多的职员指引，非常顺畅。\n我们的第一晚旅店订在Hotel Rocore Naha，距离单轨铁路县厅前站步行5分钟左右的地方，和第二天的旅行团集合地点就隔一个十字路口，据说这家店的自助早餐非常棒，但是由于时间关系，这次没有选择吃他们家的早餐。入住很简单，刷一下护照就可以。\n到冲绳的第一餐选择了在日本的大众点评（Tabelog）上面评分比较高的疾風ホルモン公式ホームページ 烤肉店，一家很有日式居酒屋风格的店铺，去之前在Line上面和店主提前预约了，可以用翻译软件翻译成日语。这个店铺离国际通很近，步行5分钟左右就到了。\r到的时间还早，店里没有其他食客，可以随便拍拍。\n点餐是扫码点餐，比较方便，也有英文中文菜单，点了一个7000日元的套餐，另外还有一杯茶，套餐包含：一小碗牛肉咖喱饭、三个泡菜、三个凉菜、三份不同部位的牛肉、一份牛内脏、一份凉面、一份甜点。约合人民币400一个人，可以说吃的非常饱了。\n自己的烤肉没啥水平，就一个字，好吃。\n这家店是等你吃完一份再上另一份的，所以可以慢慢吃，不着急。最后结账也可以刷银联或者VISA。店员超级热情，还送到电梯口，真的有点尴尬，哈哈。\n如果吃完时间还比较多的话，可以去逛一下国际通，由于第二天还有团，就早早的回旅店休息了。\nDay2 这一天的主要内容是跟团冲绳北方一日游，选择的是KKDay的冲绳中北部一日游 美丽海水族馆4小时停留、古宇利岛、万座毛、美国村｜中文导游旅行团，体感很不错，导游石川姐是早年间嫁入冲绳的中国人，非常负责，讲解当地历史文化和各个景点的内容，最关键的是，不像国内团会拉去买买买，也没有恶心的PUA消费内容，纯玩团。\n万座毛，是本次旅行团的第一站，非常小的一个景点，最出名的就是那个大象鼻子（日版象鼻山）。\n还有远处的钓鱼佬。\n古宇利岛是第二个景点，这里停留时间比较短，没能来得及去海洋塔，不过只在桥上拍海就已经觉得足够震撼了。\n下面是在桥上拍的海景。\n水族馆是本次行程的最后一站，最出名的就是海豚表演、鲸鲨喂食和大水族箱。需要注意时间，这次时间关系就没有去看鲸鲨喂食，选择去园区其他地方逛了一下。\n结束了一天的行程，最终选择住在美国村的DoubleTree by Hilton Okinawa Chatan Resort，很幸运能赶上美丽的夕阳。\n晚上选择乘出租车去Aeon Mall Okinawa Rycom吃饭和采购，大约2000日元的出租车费，这里要免税的话需要买够5000日元才可以，需要带上护照以方便办理手续。\n在永旺第一站要去顾客服务中心领取500日元优惠券，然后谨记晚上9点停止办理退税，而且需要注意的是日本有些店员年龄偏大，处理退税会有些慢，所以采购的话预留时间还是得充足一些的。\nDay3 这一天上午是纯纯的回血，没安排任何活动，睡到自然醒然后去吃自助餐，冲绳拉面的味道还是很不错的，红烧肉炖的软烂，汤底比较浓郁，很适合北方人口味。\n昨天到的太晚，没能好好欣赏一下周围风景，在阳台看出去，外面的景色还是很不错的。\n中午乘车（大巴包车）去市区，车票是在KKDay购买的那霸市内往返美国村 LeaLea OKINAWA 接驳巴士，入住 Hotel JAL CITY Naha酒店。下午体验了美樱 Chura Sakura 浴衣＆和服体验，去了波上宫。\n换装结束，决定去濑长岛碰碰运气，看看能不能赶上另一波夕阳。乘坐单轨到赤岭站，换乘公交TK02，刷VISA卡上车，一站就到，非常方便。\n很幸运，虽然云层很厚，还是抓到了夕阳。\n圣诞将近，来一张岛上的圣诞气息。\n还有一只大肥猫。\n拍完回去吃饭，在全家便利店买了冲绳特色菜，苦瓜炒蛋、荞麦面，还有 Blue Seal 的冰淇淋，味道真的很不错。\nDay4 这一天和昨天一样，只不过是上午躺，下午回家了。这家酒店的早餐很日式，试了一下味增汤，再也不会吃第二次\u0026hellip;\n在机场看到了各样的飞机和军机，在国内想拍到机场的军机基本不可能。\n冲绳机场的香港航空职员似乎是雇佣的机场工作人员，所以他们都是讲英语日语的，安检人手比较少，还是得尽快过去才好，在登机口附近也可以买免税商品，价格和在市区买的一样，推荐虾饼，味道很不错。\n一路飞回香港，入香港使用护照，需要填写入境单，乘坐机场快线到西九龙坐高铁回广州，需要注意，西九龙网上售票提前45分钟截止，所以需要预留足够时间。这里出香港需要刷护照，而进大陆海关则需要刷港澳通行证，一定不要搞错。\n最后，感觉很棒，希望明年香港机场继续搞免费机票活动！\n","permalink":"https://boringleric.github.io/posts/life/okinawa_travel/","summary":"第一次出国，冲绳4日游","title":"冲绳4日游"},{"content":"Deepspeed多机多卡训练记录\n如果用到fp16等半精度训练，需要安装nvidia的apex。\n步骤：\n先让两台机可以ssh通信，在主节点执行ssh-keygen，可以一直回车，生成的密钥在/home/你的用户名/.ssh/下，需要把公钥id_rsa.pub传到子节点的对应.ssh文件夹下，并且重命名为authorized_keys，私钥保留在本地即可。\n一定要注意：主节点上也需要将id_rsa.pub 拷贝成authorized_keys！！！\n随后可以ssh连接子节点测试一下，不需要输入密码也可以连接。\n在系统层面安装pdsh来帮助nccl通信，在我这里使用的debian直接apt安装就可以了。\n我这里主从系统配置了完全一样的系统、conda环境、conda虚拟环境名称，代码路径，代码完全保持一致。\n配置hostfile，格式为：192.168.x.xx slots=2；前面是标注机器，注意主机要放到第一行，slot代表每台机显卡数量。\n可以通过DeepSpeed/DeepSpeedExamples/pipeline_parallelism这个例子进行单机多卡、多机多卡测试，区别是多级多卡测试需要加一行\u0026ndash;hostfile=hostfile。\n遇到的问题：\ncannot import name \u0026lsquo;UnencryptedCookieSessionFactoryConfig\u0026rsquo; from \u0026lsquo;pyramid.session\u0026rsquo; (unknown location)\n问题定位：不要pip install apex，这不是nVidia的apex，需要去github下载。\n解决方法：GitHub - NVIDIA/apex: A PyTorch Extension: Tools for easy mixed precision and distributed training in Pytorch 在这里安装。\nModuleNotFoundError: No module named \u0026lsquo;fused_layer_norm_cuda\u0026rsquo;\n问题定位：apex和cuda/pytorch不匹配，apex代码错误等都有可能出现。\n解决方案：我这里是根据github的apex issue重新下载了符合pytorch版本的apex版本重新编译就好了。\n​\t3. proxy call to rank 1 failed (connect)\n​\t问题定位：NCCL机器通讯问题\n​\t解决方案：\n​\texport NCCL_DEBUG=INFO ​\texport NCCL_IB_DISABLE=1 ​\texport NCCL_SOCKET_IFNAME=网卡名字\nNinja is required to load C++ extensions ​\t问题定位：Ninja找不到path中的C++库位置\n​\t解决方案：\nimport os new_env = os.environ.copy() new_env [\u0026#34;PATH\u0026#34;] = \u0026#34;/本地conda环境/bin:\u0026#34; + new_env [\u0026#34;PATH\u0026#34;] os.environ.update(new_env ) ","permalink":"https://boringleric.github.io/posts/tech/deepspeed_multinode_multicard/","summary":"一次LLM多机多卡训练记录","title":"Deepspeed多机多卡训练记录"},{"content":"GPT系列一直坚持做基于Transformer Decoder的生成模型，其规模也是越来越大，终于在2022年底，chatGPT的诞生，将NLP预训练模型拉到了一个新的高度。\n对于超大模型，之前也没有什么体验，由于缺乏算力，只能在理论上对这些内容进行一些整理，主要讲一些学习方法：\nPrompting Prompting方式，又称为提示方式，实现大模型的few-shot/zero-shot learning的一种好方法。由于finetune所需的数据可能较难获得，基于大模型的prompt是一种让预训练模型直接适应下游任务的方式，具体做法是人为构造一批规则数据，让模型补全结果，将补全结果作为输出。prompt 生效的最基本前提是预训练语料已经覆盖了任务语料的分布。\n举一个例子：在文本情感分类中，希望得到如下语句“这个电影情节不错，节奏紧凑，看得很过瘾。”属于正向/反向情感，可以构造如下prompt：“上述电影看起来很__”，让模型填充可以表现情感的答案，如好/不好，甚至更细粒度的，如有趣/无聊等。\nPrompt和finetune的区别在于，finetune是语言模型去适应某个下游任务，引入各种loss去调整模型参数，对于分布外的鲁棒性可能有一定的损失；而prompt是重构下游任务来适配语言模型，所需参数更少，对于不同分布情况下的学习效果更好，但是需要找出合适的prompt来解决对应的问题，这也算一种新的编程范式了。\n从检索的角度讲，prompt可以看作是对语言模型记忆知识的一个检索，相对于finetune，prompt更接近于预训练语言模型任务，所以相对可以得到更多的内在信息，也就是更好的应用于few-shot任务的原因。\nIn-context learning In-context learning又称为上下文学习，也就是基于prompt提供的几个例子，然后模型根据这些例子做生成，非常类似于无监督预测，但在输入测试样例前引入少量标注前缀数据，不需要参数调整，直接训练。\n那么模型在引入的前缀数据中学习到了什么呢？在论文Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?中得出的结论有：\n模型没有学习输入和标签之间的关联； 模型学到了输入数据、预测标签的分布，以及这种数据+标签的表达形式，但是知识使用的是模型在之前训练中学习到的； 模型没有真正意义上建模输入输出样本之间关联的学习，只时使用了前缀来激活大模型语言表达的建模能力。 思维链 思维链 (chain-of-thought)，是prompt的一个衍生型，也就是写prompt的时候，不仅需要输出结果，还需要给出结果推理出来的过程。\n需要注意的是，上述的内容都是使用大模型得到的结果，当模型参数较少时，这种训练方式远不如finetune得到的收益高，而当模型参数提升时，in-context learning得到的收益大大提高。模型至少需要62B参数，思维链的效果才能大于Prompt方法，至少需要 175B（GPT3），思维链的效果才能大于精调小模型（T5 11B）的效果。\nRLHF RLHF的全称是基于人类反馈的强化学习(Reinforcement Learning from Human Feedback) ，也是chatGPT强大的原因。\nRLHF主要分三个阶段，其中使用人工标注的前两个阶段就是所谓“人类反馈”：\n冷启动阶段监督策略模型：从用户提交的 prompt 中随机抽取一批，通过专业标注人员给出指定 prompt 的高质量答案，然后用这些人工标注好的 (prompt,answer)对来 finetune 模型，让模型具有基础的理解能力； 训练回报模型（Reward Model,RM）：随机抽样一批用户提交的 prompt，然后使用第一阶段训练好的模型为每个 prompt 生成 K 个不同的回答，再让标注人员对 K 个结果进行排序，以此作为训练数据，通过 pairwise 方法，learning to rank 模式来训练回报模型； 采用强化学习来增强预训练模型的能力：利用上一阶段学好的 RM 模型，靠 RM 打分结果来更新预训练模型参数。 RLHF不会为模型注入新的能力，所有的能力都是在预训练过程中得到的，RLHF的作用是解锁 / 激发这些能力，因为RLHF的数据量比预训练数据量少几个数量级。RLHF的学习可以让模型生成更加符合人类期待的反馈，如生成安全对话、拒绝超出模型它知识范围的问题等。\n当前大模型存在的问题 目前大多数prompt和finetune的对比都是prompt+大模型对比finetune+相对小模型，并不属于相对公平对比，且base模型也并不一致； 难以更改模型内部知识； 无法严格推理，无法检索网络。 ","permalink":"https://boringleric.github.io/posts/tech/simple_chatgpt/","summary":"Chatgpt太火了，简单了解一下。","title":"Chatgpt的简单了解"},{"content":"Bert模型虽然很强大，也有一些不足，比如：\n预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务； 没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计； 最大输入长度的限制，适合句子和段落级别的任务，不适用于文档级别的任务。 所以自Bert模型之后，各种基于Transformers结构的模型喷涌而出，它们有的试图结合多模态增强学习能力，有的试图在Attention方面做出改进以提高效率，下面将会对部分改进模型进行展示。\n针对mask策略优化 Roberta RoBERTa模型主要是Bert模型的超级调参优化版本，主要有如下修改点：\n增加了训练数据，修改了超参数，加大batch size、训练步数； 在更长的序列上进行MLM训练，并移除NSP任务； 将Bert的静态Mask改为动态Mask，动态Mask离线，将同一份数据掩码10次，达到动态掩码的作用； 采用更大的byte-level的BPE词典，变成了50000个token； 加入了混合精度训练模型。 针对中文，加入了全词掩码（WWM）。\nERNIE 1.0版本引入知识图谱信息，知识图谱实体一般是人名、地名、机构名等，因此采用多种掩码机制，保留BERT原有的字符级别掩码，引入实体级别掩码和短语级别掩码来增加知识图谱信息。\n增加DLM任务：处理百度贴吧的线程数据、通过用随机选择的句子替换查询或响应来生成假样本。该模型用于判断多回合对话的真假。\n2.0引入了更多的任务，包括：知识掩码任务、大写预测任务、 词语-文档关系预测任务、句子重排序任务、句子距离预测任务、句子关系任务、检索关系任务。\n其中，知识掩码任务，就是ERNIE1.0的掩码任务；大写预测任务，就是预测一个英文词语的首字母是否为大写，一般大写词语都是较重要的词语；词语-文档关系预测任务，就是一篇文档出现的词语，是否在另外一篇出现；句子重排序任务，就是将文本句子打乱顺序，然后预测正确排序；句子距离预测任务，就是预测两个句子之间的距离；句子关系任务，就是根据句子之间的关键词判断，两个句子之间的语义关系；检索关系任务，就是判断一条用户查询和一篇文档题目的相关性程度，包含强相关、弱相关和无关。\n使用ALBERT提出的句子顺序预测（SOP）任务替换BERT原始的NSP任务，通过切换两个连续句子的原顺序创建负样本。\n3.0设计了一个连续的多范式统一预训练框架，以实现多任务范式间的协同预训练。低层是所有任务共享的，而顶层是特定任务的，使不同的任务范式能够共享在一个共享网络中学习的底层抽象特征，并分别利用在特定任务网络中学习的特定任务顶层具体特征。\nNezha NeZha模型的主要改进点包括：\n增加相对位置编码函数，可以在推理时接收更大的长度； 全词掩码； 混合精度训练； 使用LAMB优化器：LAMB 优化器支持自适应元素级更新和准确的逐层修正，可将 BERT 预训练的批量大小扩展到 64K，且不会造成准确率损失。 SpanBERT SpanBert提出了span mask方案，和span边界预测任务，并且同样舍弃NSP任务。\n使用随机Span片段进行Mask操作，而不是Mask特定词汇，避免引入词边界信息。先根据几何分布随机选择span长度，再根据均匀分布随机选择起始位置； Span Boundary Objective任务，在训练时获取Span掩码边界的前后两个token（注意，不是边界token，是边界前后的token），然后将这两个token的向量加上 Span被掩掉token的位置向量，来预测被掩掉token的原词是什么，就是将词向量和位置向量拼接起来，经过全连接层进行词表预测； Mask token的占比依然为15%，并按照811的规则替换。 Span掩码的长度是如何选取的？\n设定长度为10，p为0.2的几何分布。10之后的长度丢弃，因此，后面的概率会分布到前面1-10的概率上，与正常几何分布概率不同。\nspan长度为1 的概率是 $\\frac{p}{1 - {q^{10}}} = \\frac{0.2}{1 - {0.8}^{10}} \\approx 0.224$。\nspan长度的期望计算过程为： $$ E(x) = p(1 + 2q + 3{q^2} + \u0026hellip; + 10{q^9}) = \\frac{1 - q^{10}}{{(1 - q)}^2} - \\frac{{10q^{10}}}{1 - q} \\approx 0.2 \\cdot 16.946 \\approx 3.8 $$\nMacBERT 主要改进点：\nWWM和N-gram Masking引入：单个token、2-gram、3-gram、4-gram分别Mask的比例为0.4、0.3、0.2、0.1； 避免Mask token影响finetune任务：使用同义词工具包Synonyms获得相似的词。如果被选中的N-gram存在相似的词，则随机选择相似的词进行替换，当没有相似的词时，使用随机词替换； UniLM 这个模型其实应该算大一统模型，但是其仍使用Bert架构，只是在mask矩阵的角度出发进行改进，提出了单向、双向和seq2seq三种特殊的Mask预训练目标，让模型在NLU任务稳定情况下可以做NLG任务，与其他生成类模型使用seq2seq结构不一致。\n通过不同的mask来控制预测单词的可见上下文词语数量，实现不同的语言模型的联合训练：\nUnidirectional LM：分为从左到右和从右向左两种，从左到右，即仅通过mask token的左侧所有token来预测mask token；从右到左，则是仅通过mask token的右侧所有token来预测mask token。 Bidirectional LM：与BERT模型一致，在预测mask token时，可以观察到所有的token。 Seq2Seq LM：如果mask token在第一个文本序列中，那么可以使用第一个文本序列中所有token，不能使用第二个文本序列的任何信息；如果mask token在第二个文本序列中，那么使用一个文本序列中所有token和第二个文本序列中mask token的左侧所有token预测mask token。 token mask的概率，与BERT模型一致。此外，在80%的情况下，每次随机mask掉一个token，在剩余的20%情况下，mask一个二元token组或三元token组。\n训练UniLM模型时，使用1/3的数据进行双向语言模型优化，1/3的数据进行序列到序列语言模型优化，1/6的数据进行从左向右的单向语言模型优化，1/6的数据进行从右向左的单向语言模型优化。\n训练时每个batch为一个任务，2个双向语言模型任务，2个序列到序列语言模型任务，1个左向右的单向语言模型任务，1个从右向左的单向语言模型，每跑一个任务进行一次累计梯度，跑完一轮所有任务，执行一次反向传播。\n自监督目标 ELECTRA Electra提出了一个新的模型预训练框架，采用generator-disciminator结合的方式，但是又不同于GAN，同时将Bert的MLM方式改为replaced token detection。\ngenerator模型就是一个小的transformer encoder，通常是1/4的discriminator大小。这一部分的训练仍然是经典的MLM任务，但是分布方式并非811，而是选择15%的token转为mask，随后进行MLM训练； discriminator模型依然是transformer encoder结构，使用generator的embedding信息，主要负责判断generator传入的序列每一个token是原始token还是被修改过的token，相比MLM任务，这个任务更加高效，并且训练好的判别器模型会用于下游任务。 训练步骤：首先随机mask输入序列；接下来输入到生成器，生成器预测mask token，得到新序列；之后将新序列输入判别器，对每一个token判断是否原token，最后将二者loss加和分别反向传播，另外由于判别器的任务相对来说容易些，RTD loss相对MLM loss会很小，因此加上一个系数，论文提供的参数为50。 为什么不直接使用随机替换？测试显示存在生成器模型的原因是直接使用随机替换的词语效果不好。\n计算效率：训练判别器分辨每一个 token 是否被替换，而不是 MLM 那样训练网络作为生成器预测被损坏的 15% token 的原始 token，从而使模型从所有的输入 token 中学习而不是 MLM 中那样仅从 masked 的部分 token 学习。此外，生成器与判别器共享token embedding以及判别器使用的二分类而非Vocabsize分类任务也有效提升了训练效率。\n参数效率：最终得到的判别式分类器不必对完整的数据分布进行建模。但Electra不是GAN模型，句子的字词是离散的，梯度在判别器使用生成器结果时就断了，判别器的梯度无法传给生成器，生成器的训练目标还是MLM。\n生成器和判别器的权重共享分析：在相同参数下（生成器和判别器结构一致），不共享权重下的模型效果最差，共享所有权重的效果最好，只共享token embedding层的效果只比共享所有权重差一点点。原因是生成器是一个MLM任务，在模型预测时softmax建立在词典的所有词之上，反向传播会更新所有token 的embedding，因此生成器对token embedding层的学习效果更好。最后论文作者只使用了token embedding共享策略。并且实验发现生成器的大小在判别器的1/4到1/2之间效果是最好的。\nDeBERTa Deberta模型更新了3代，有较多的创新点，主要有下面几条：\ntoken embedding解耦，在bert中，每一个token只用一个向量表示，这个向量结合了word embedding，position embedding和segment embedding。但是deberta将token embedding进行解耦，用content向量$H_i$和relative position向量$P_{i|j}$来表示token i和相对位置j。 两个token之间的attention score计算：token i 和 token j之间的attention score可以被分解为 4 个部分 (content-to-content, content-to-position, position-to-content and position-to-position)，用公式表示为：${A_{i,j}} = { {H_i},{P_{i|j}}} \\times {{ {H_j},{P_{j|i}}} ^T} = {H_i}{H_j}^T + {H_i}{P_{j|i}}^T + {P_{i|j}}{H_j}^T + {P_{i|j}}{P_{j|i}}^T$，作者认为内容相关注意力很重要，位置到内容也很重要，因为单词对的注意力权重不仅取决于它们的内容，还取决于相对位置，但正因为全部采用相对位置编码，所以最后一项并不是很重要。由于采用了相对距离嵌入向量，DeBERTa 可以处理任意长度的输入向量。 使用对抗训练算法 Scale-invariant-Fine-Tuning (SiFT)来微调系统，通常对于NLP任务的对抗训练加在word embedding上，但是不同token对应的 word embedding的norm各不相同，并且模型参数越大，norm的方差也就越大，这会使得训练过程不稳定。SiFT先将word embedding归一化为概率向量，然后在归一化的word embedding上添加扰动。 增强掩码解码器（enhanced mask decoder）替换原始输出的softmax层，以预测模型预训练时被mask掉的token，来避免预训练任务和finetune任务不一致问题，也就是包含一个或多个 Transformer 层再接 softmax；另外将Encoder的输出送入Decoder时，将mask token中10%不改变的token 编码换成其绝对位置embedding，然后再用MLM预测。因为这些token虽然不会造成预训练和精调阶段的不匹配，但是却导致token泄露本身信息。 在v2模型中使用sentencepiece tokenizer；添加了额外的卷积层来学习更好的token之间的依赖关系；共享attention层的位置投影矩阵与内容投影矩阵；引入T5的bucket概念编码相对位置。 在v3模型中使用了Electra的RTD预训练任务取代了MLM预训练任务，electra的共享参数影响了模型表现能力，因为MLM倾向于使得语义相近的tokens对应的embed也比较接近，而RTD倾向于使得语义相近的tokens对应的embed相互远离以方便区分；但是若不共享token embedding，模型性能会有所下降，影响discriminator的性能，并且两个任务loss差距非常大，导致更新效果不佳，所以设计了Gradient-disentangled embedding sharing (GDES)，共享token embeddings，阻止判别器的梯度反向传播到生成器的embedding，只使用MLM的loss而不使用RTD更新生成器，使得训练更高效，并引入一个全零矩阵适配token embedding。 v3的一个训练迭代流程，GDES先用前向+后向的MLMloss更新生成器，并更新共享的token embedding，随后再用前向+后向的RTDloss更新全零token embedding。 coco-lm coco-lm和上述结构类似，主要在预训练层面改动，将mlm任务替换为纠正语言建模(Corrective Language Modeling)，并引入序列对比学习（Sequence Contrastive Learning ）。\nCLM任务通过网络模块不断扰乱破坏原始文本输入，让模型不断恢复原始语句通过鉴别模块的鉴定，从而学习到原始语句细节语义，增强模型泛化能力。CLM结合了主Transformer模块在二分类任务的帮助下对all tokens进行训练，同时也能够预测单词，从而享受ELECTRA的效率优势，并保留语言建模的优势。\nSCL在序列输入的基础上形成一个对比学习目标，以学习更鲁棒的表示为目的。广义上说，对比学习是将一对正向的实例与不相关的负向实例进行对比，正例样本通常是通过对相同的输入进行数据增强来获得，以增强神经网络鲁棒性，将不相关的序列在表示空间中分开，并确保随机数据点之间的低余弦相似度，从而获得更好的对齐和均匀性提高泛化能力。\n大一统模型 想做大统一的模型可以从两方面着手，第一是任务混合，第二是结构混合。\n任务混合指将NLU和NLG同时进行预训练，比如XLNet采用的Permutation language modeling，兼顾上下文与自回归；或者是UniLM采用的Multi-task training，设计注意力矩阵同时训练多个任务。\n结构混合主要使用Seq2seq去做NLU和NLG任务，如MASS、T5和BART。主要问题是参数太多，训练效率低，且NLU任务效果较差。\nXLNet XLNet的主要创新点Permutation Language Model，在auto regression的场景下采用双流自注意力机制，融入双向语言模型，并且使用Transformer-XL的encoder。\n双流自注意力机制的描述流程如下图所示：\n其中a图表示content流自注意力，是标准的self-attention过程，只是为了不使用mask标记符号，引入了b图表示的query流自注意力，query stream用g表示，content stream用h表示，query流对要预测的位置进行计算时，query向量使用g计算获得，包含该位置的位置信息，但k和v使用h计算，包含其他token的内容信息。\n根据右侧的3-2-4-1排列计算，可以看出在计算token1的q向量时，只使用了token1的query流，但向量kv可以使用234token的h信息，从mask矩阵可以看出，对角线上的当前位置信息都被mask掉了。\ncontent流的计算就是标准的self-attention操作，和 Query流的掩码矩阵区别在于对角线，Content流不遮掩对角线，使得当前 token 的信息可以传递到下一层。\nTransformer-XL主要使用相对位置编码以及分段RNN机制来增强长文本学习能力。\nMASS MASS采用Encoder-Decoder结构，借助k超参数，提供了通用训练框架，模型在编码器端输入一个被随机掩掉长度为k的连续片段的句子，然后通过解码器预测被掩掉的长度为k的连续片段。\n当k=1时，编码器端掩掉一个token时，解码器端也仅预测一个token，这时的MASS模型和BERT模型的预训练方法相似：\n当k=序列长度时，编码器端掩掉所有token，解码器端预测所有token，这时的MASS模型和GPT模型的预训练方法相似：\n训练过程中，在编码器端没有被掩掉的词，在解码器端都被掩掉，促使解码器需要从编码器端提取更多的信息来生成连续片段，这样促进了编码器-解码器结构的联合训练；并且在训练时，50%token不需要预测，可以节省50%的时间。\nT5 T5也是一个通用的框架，使用Encoder-Decoder结构，大力出奇迹，将所有任务都转成一种模式：text2text。也就是说同样的模型、损失函数、训练过程、解码过程来完成所有的NLP任务。\n将原始文本的片段进行MASK，并用特定的字符进行占位，将其输入到编码器中；解码器为连续输入特定的占位符，预测其原始文本内容。\n策略选择如下图所示，主要从四个层面比较：\n从预训练方法对比，有语言模型任务（GPT2）类型，从左往右预测；有Bert类型也就是MLM任务；还有顺序还原类型，也就是将文本扰乱并复原，最终选择Bert类策略； 从文本破坏策略对比，有mask法，只替代一个token；也有span策略，将一段token用mask替代；还有就是drop法，不替换，直接丢弃，最终选择span策略； 在破坏程度选择上面，选择了和bert相同的15%策略； 在片段长度选择策略中，长度为3时效果最好。 其他tricks：增大模型最有必要，数据要洗好，代码层面T5没有scaled，去掉了Layer Normalization的center操作，并去掉每一层的bias项。\nBART BART也是使用Encoder-Decoder结构的模型，其预训练是使用多种噪声对原始文本破坏，通过seq2seq再重建原始文本，因此损失函数为decoder的输出与原始文本的交叉熵。\nBART的破坏操作:\nToken Masking：与BERT一致，随机抽取token，并用[MASK]标记进行替换； Token Deletion：从输入中随机删除token，与掩码不同，该策略为了让模型学习哪些位置缺少输入信息； Text Infilling：随机挑选一个文本片段，长度符合$λ = 3$的泊松分布，并且使用一个[MASK]标记进行替换。当片段长度为0时，相当于在原始位置插入一个[MASK]标记。与SpanBERT模型不同的是，SpanBERT模型是使用片段长度个数的[MASK]标记进行替换； Sentence Permutation：将文本按照句号进行分割，生成句子序列，然后将句子之间的顺序随机打乱； Document Rotation：随机选择一个token，然后将文本进行旋转，即以选择token作为文本的开头。 BART最终使用了Text Infilling策略和Sentence Shuffling策略的组合，屏蔽30%的token并排列所有的句子。\n模仿人类认知能力（Cognitive-Inspired Architectures） 事实上，人脑远比Attention复杂的多，想达到人类智能，还需要决策能力、逻辑推理能力、反实时推理能力。为了具备以上几种能力，需要模型有短时记忆与长期记忆，短时记忆用来决策和推理，长期记忆用来回忆事实和经验。\n像Transformer-XL，CogLTX这类模型，通过样本维度的记忆提升长距离理解能力，实现推理；REALM这类模型则是对语料、实体或者三元组进行记忆，将信息提前编码，在需要的时候检索出来。\nTransformer-XL Transformer对于较长的序列建模能力有限，如bert支持的序列最大长度是512，超过了该长度的序列需要进行截取，再把截取后的片段分别用bert进行编码，存在上下文碎片化的问题，互相之间没有上下文信息，并且，片段位置编码都是从0开始，信息丢失严重，所以引入了Transformer-XL。\nTransformer-XL主要提出了两个创新点：1. Segment-Level Recurrence Mechanism 段级递归；2. Relative Positional Encodings 相对位置编码。\n传统transformer对于长文本分段如下图所示，训练过程中两个segment没有相互依赖，推理过程需要使用滑动窗口计算。\nTransformer-XL使用递归机制，第一个segment计算完成后保存结果，计算第二个片段时将第一个segment的隐状态和第二个segment的隐状态拼接再计算结果。\n关于相对位置编码，Vanilla Transformer使用的是绝对位置编码，不同的片段的同一个位置其位置编码都是一样的，模型不能正确区分不同片段的位置信息。在算attention score的时候，只考虑query向量与key向量的相对位置关系，并且将这种相对位置关系，加入到每一层attention的计算中。相对位置关系用一个位置编码矩阵表示，第i行表示相对位置间隔为i的位置向量。矩阵采用正弦函数生成，而不是通过学习得到的，预测时可以使用比训练距离更长的位置向量。\nCogLTX Bert在长文本处理一般分为三种方法：截断法，如Transformer-xl；Pooling法；压缩法，也就是本文的cogLTX。\ncogLTX有一个假设：存在短文本$z$可以表示原长文本$x$的语义。那么如何找到这个短文本呢：\n使用动态规划将长文本$x$划分为文本块$[x_0,\u0026hellip;,x_{T-1}]$； 使用MemRecall对原长句中的子句进行打分，结构如下图所示，选出分数最高的子句组成$z$之后再进行训练，这样cogLTX相当于使用两个bert，MemRecall的bert负责打分，另一个bert执行原本的NLP任务。 REALM REALM提出了一种更加模块化且可解释性更强的知识嵌入方法。训练一个独立的上下文知识抽取器（contextual knowledge retriever）来决定应该在推理时使用哪些知识，同时这个抽取器和语言模型一起进行非监督预训练大大提高模型性能。\n使用多源数据 跨语言学习，多模态学习，如OpenAI 的CLIP 和 DALL・E将 NLP与图像识别结合在一起，可以更好的完成文本生成图片任务。\n效率改进 对模型的压缩和加速是有区别的，压缩侧重于减少网络参数量，加速侧重于降低计算复杂度、提升并行能力等，压缩未必一定能加速 主流的压缩与加速技术有4种：结构优化、剪枝(从网络中去掉不必要的部分。包括权重大小剪枝、注意力头剪枝、网络层以及其他部分的剪枝等。还有一些方法也通过在训练期间采用正则化的方式来提升剪枝能力)、量化(FP16，INT8)、知识蒸馏。\n轻量化方式 具体方法 矩阵分解 使用SVD，Flattened Convolutions等轻量化模块 模型剪枝 舍弃模型影响较小的部分，Weight Prunning，Pruning with rehabilitation，Hessian-based method 模型量化 使用低精度来表示高精度模型，整数，binary替换 参数共享 相似模型单元间的参数共享 知识蒸馏 通过一些优化目标从大型、知识丰富、fixed的teacher模型学习一个小型的student模型 模块替换 根据伯努利分布进行采样，决定使用原始的大模型模块还是小模型 Attention效率改进：GAU 标准attention的复杂度是$O(n^2)$级别，n是序列长度，所以当n比较大时，transformer模型的计算量难以承受。制约Attention性能的关键因素，其实是里边的softmax，若没有softmax，三个矩阵连乘，根据结合律可以得到$d \\times d$的矩阵，由于$d\u0026laquo;n$，所以复杂度可以降到$O(n)$级别。\n对于attention的优化时间复杂度改进思路主要有稀疏化和线性化两种套路：\n稀疏化attention主要有：Reformer（通过LSH将Attention复杂度降到$O(nlogn)$），还有一些跟Pooling结合的如Linformer(使用两个矩阵对KV分别投影，减少矩阵规模)也可以理解为广义的稀疏化。这类工作的特点是引入一定的归纳先验，强制大部分注意力为0，从而理论上减少计算量，缺点是往往需要专门的编程优化才能实现加速，或者是难以用来做Decoder（Pooling类工作），此外效果好坏比较依赖于其引入的归纳先验，显得不够自然。\n线性化attention有：Performer（通过随机投影，在不损失精度的情况下，将Attention的复杂度线性化，效果一般）、Nyströmformer（基于矩阵分解的线性化Attention）这类工作是将标准Attention的$\\phi (Q{K^T})V$改为${\\phi _q}(Q)({\\phi _k}{(K)^T}V)$从而实现线性复杂度，好处是易于实现，但有存在低秩性会导致效果明显变差；另外是用来做Decoder时会牺牲训练并行性，因为它需要转化为RNN来计算，又或者不牺牲并行性，但需要$bhns^2$的空间复杂度，相比于标准Attention的$bhn^2$，起码要$n≫s2$才有优势，而哪怕$s=64$，都要$n≫4096$了，多数情况下不现实。\nFlash提出了一种新的Transformer变体，它依然具有二次的复杂度，但是相比标准的Transformer，它有着更快的速度、更低的显存占用以及更好的效果；提升了原有线性Attention的效果，还保持了做Decoder的可能性，并且做Decoder时还能保持高效的训练并行性。\n门控注意力单元GAU融合了GLU（门控线性单元）和Attention，显示了单头注意力未必就逊色于多头注意力，并提出注意力未必需要Softmax归一化，可以换成简单的relu2除以序列长度。\nFLASH采取了局部-全局分块混合的方式，结合了稀疏化和线性化的优点。首先，对于长度为n的输入序列，将它不重叠地划分为n/c个长度为c的块，将每个块通过仿射变换得到新矩阵，计算新矩阵块内自注意力，接下来按照之前的线性attention方法计算，最后，将两种Attention结果结合起来，整合到GAU中，得到线性版本的GAU。\n之所以这样分块做“局部-全局”的混合注意力，除了是想降低计算成本外，还因为这样做能得到更贴合实际情况的注意力分布。按照对NLP的经验理解，自然语言中的关联主要还是集中在局部的，而全局的、极度长距离的关联虽然存在，但不会是主导地位，所以这种混合式的注意力设计更有利于模型凸出局部关联但不舍弃长程关联。\n但GAU里的不进行概率归一化的Attention设计可能存在外推能力欠佳的问题。\n矩阵分解：Albert ALBERT模型主要采用了词嵌入向量参数的因式分解，权重共享的方法。主要有以下几点：\n权重因子分解：将参数矩阵分解成两个较小矩阵的乘积来逼近原始参数矩阵。给矩阵施加了低秩约束。权重因子分解既可以应用于输入嵌入层（节省磁盘），也可以应用于前馈/自注意力层的参数（提高速度）。\n权重共享：模型中的一些权重与模型中的其他参数共享相同的值。\n使用句间连贯性损失：NSP任务对模型的预训练并没有太大的帮助，正例样本是正常顺序的两段文本，而负例样本是将两段文本的顺序进行颠倒。\n去掉dropout，使用LAMB训练器，使用ngram做MLM。\n最大的问题就是这种方式其实并没有减少计算量，也就是推理时间并没有减少，训练时间的减少也有待商榷。\nTensorRT优化 目标是提高GPU利用率，提升推理效率，降低延迟：\n层间融合/张量融合：在构建engine阶段完成，横向合并可以把卷积、偏置和激活层合并成一个CBR结构，只占用一个CUDA核心。纵向合并可以把结构相同，但是权值不同的层合并成一个更宽的层，也只占用一个CUDA核心，通过减少核函数调用次数来提高 GPU 利用率；\n模型量化：在构建engine阶段完成，FP32降为FP16或INT8，更低的数据精度将会使得内存占用和延迟更低，模型体积更小；\nCUDA核自动调整：在推理阶段，TensorRT 会在目标 GPU 卡上选择最优的层和并行优化算法，保证最优性能。\n动态申请 Tensor 显存：在推理阶段，Tensor 使用时再真正申请显存，避免显存重复申请，提高显存利用率；\n并行多流执行：在推理阶段，通过共享权重的方式并行处理多条任务流，优化显存。\n知识蒸馏：DistillBERT，TinyBERT，MobileBERT 知识蒸馏可以将一个模型的知识转移到另一个模型，两个模型可以是同构或者异构。一般是先训练一个教师模型，然后使用这个教师模型的输出和数据的真实标签去训练学生模型，目标是提高推理速度，减少显存使用率：\n设计学生模型：可以通过减少模型的层数（高度），或者通过减少模型的宽度来实现，DistillBERT 和 PKD-BERT 只减少了模型的层数，而 MobileBERT 只减少了模型的宽度，而 TinyBERT 既减少了模型的层数也减少了模型的宽度。早期的工作只关注层数的减少，这样做的好处是可以直接使用教师模型中的权重来初始化学生模型，但是减少层数对模型的压缩毕竟是有限的。而减少模型的宽度则意味着无法直接使用教师模型的参数对学生模型进行初始化。此外，MobileBERT 指出减少了宽度，attention head 数目也应该减少； 设计目标函数：无论使用何种模型压缩方法，模型输出和真实标签之间计算的交叉熵损失都是最基本的损失函数。让学生模型和教师模型在输出的 logit 上尽可能相似，并使用了 T-softmax 和 KL 散度来实现，transformer 在每一层的输出（包括 embedding）也应该相似，同时 transformer 中的 MHA(multi-head attention)模块中输出的注意力权重也应该相似，如果教师模型和学生模型的宽度不一样那么就期望它们投影到一个相同的空间后相似。为两个句子分别计算一个句子表示，然后期望教师模型和学生模型在输出的句子表示尽可能相似。对于序列标注任务，期望 CRF(conditional random field)计算过程中的后验概率矩阵相似； 微调阶段的蒸馏：若(1) 微调阶段的知识蒸馏所得到的学生模型是否可以满足准确率/f1上的要求。(2) 削减层数是否可以达到所要求延时降低的效果；如果不能达到，那么需要减小模型的宽度，这使得从教师模型到学生模型的初始化无法进行，才考虑预训练阶段的蒸馏。 使用logit比直接类别输出的优点：\n当类的标记有误时，教师模型可以一定程度上消除这些错误； 教师模型输出的 logit 比原始给定的类标记更加平滑，容易学习； 相比于原始的值为 0/1 的类标记，logit 包含更多的信息量，特别是当类别数目很多的时候； 部分样本的类标记可能根本无法通过输入的特征学习到，这些样本会对模型的训练产生干扰，而以教师模型输出的 logit 为目标进行训练则会消除这些干扰。 细节研究 post-norm or pre-norm Transformer结构的post-norm，GPT2的pre-norm，DeepNet提出的deep-norm\npre-norm 和 post-norm 的区别：\npre-norm：${x_{n + 1}} = {x_n} + f(norm({x_n}))$，其中第二项的方差由于有 norm 是不随层数变化的，于是 x 的方差会在主干上随层数积累，深层部分实际上更像扩展了模型宽度，相对好训练，但某种意义上并不是真正的 deep； post-norm：${x_{n + 1}} = norm({x_n} + f({x_n}))$，保证主干方差恒定，每层对 x 都可能有较大影响，代价则是模型结构中没有从头到尾的恒等路径，梯度难以控制，更难收敛，但训练出来的效果更好； deep-norm：${x_{n + 1}} = norm(\\alpha {x_n} + f({x_n}))(\\alpha \u0026gt; 1)$，通过控制参数起到了一个折中的效果。 论文认为，这种不稳定性源于训练开始时**“爆炸式”的模型更新**。这会使模型陷入一种局部最优状态，增加每个LN（Layer Normalization）的输入量，通过LN的梯度会随着训练变得越来越小，从而导致梯度消失，使模型难以摆脱一开始的局部最优状态。最终破坏了优化的稳定性。Deepnorm\npost-norm在残差之后做归一化，对参数正则化的效果更强，进而模型的鲁棒性也会更好；pre-norm相对于post-norm，因为有一部分参数直接加在了后面，不需要对这部分参数进行正则化，正好可以防止模型的梯度爆炸或者梯度消失，因此，如果层数少post-norm的效果其实要好一些，如果要把层数加大，为了保证模型的训练，pre-norm显然更好一些。\n部分参考 FLASH：可能是近来最有意思的高效Transformer设计 封神榜中文DeBERTa预训练模型来了！ Deberta不再领先，微软提出新SOTA模型COCO-LM 预训练模型最新综述：过去、现在和未来 ","permalink":"https://boringleric.github.io/posts/tech/transformers_variaties/","summary":"Transformers架构的一系列优化变体。","title":"Transformer架构的一系列改进型简介"},{"content":"在目前的机器学习或者深度学习系统中，向量化已经成为不可避免的趋势，如同万物皆对象一般的万物皆向量，词可以用词向量（Word Embedding），句子可以使用句向量，甚至文章都有篇章级别的向量，但是至于这些向量的表示效果，有好有坏，这篇文章就先介绍一下词向量的相关内容。\none-hot编码词向量 从最基本的词向量开始，最容易想到的表示方法就是one-hot编码表示法，只需要一个包含所有需要的词的词表，假设词表的长度为n，则对于每一个词的表征向量均为一个n维向量，且只在其对应位置上的值为1，其他位置都是0，如下图公式所示： $$ {w^a} = \\left[ {\\begin{array}{c} 1\\ 0\\ \\vdots \\ 0 \\end{array}} \\right],{w^{at}} = \\left[ {\\begin{array}{c} 0\\ 1\\ \\vdots \\ 0 \\end{array}} \\right],\u0026hellip;,{w^{zebra}} = \\left[ {\\begin{array}{c} 0\\ 0\\ \\vdots \\ 1 \\end{array}} \\right] $$ one-hot向量非常直观的将每个单词表示为完全独立的实体，一目了然，但是这样的表征方法存在以下问题：\n有序性问题：无法反映文本的有序性。语言并不是一个完全无序的随机序列，特定的一系列词按特定的顺序组合在一起才能组成一个有意义的句子； 语义鸿沟：无法通过词向量来衡量相关词之间的相似程度，因为任意两个向量的距离是相同的； 维度灾难：高维情形下将导致数据样本稀疏，距离计算困难，加重了下游任务计算负担。 虽然one-hot向量不太好用，但是依然可以拿来构造句向量，为了更好的评价所构造的句子合理性，就引入了统计语言模型（如N-gram模型）和神经网络语言模型（Neural Network Language Model，NNLM）。\n统计语言模型 如何计算一段文本序列在某种语言下出现的概率？统计语言模型给出了解决这一类问题的基本框架，一段文本序列$S = {w_1},{w_2},\u0026hellip;,{w_T}$的概率可以表示为： $$ P(S) = P({w_1},{w_2},\u0026hellip;,{w_T}) = \\prod\\limits_{t = 1}^T {p({w_t}|{w_1},{w_2},\u0026hellip;,{w_{t - 1}})} $$ 也就是将序列的联合概率转化为一系列条件概率的乘积。这样，要求出一段文本的联合概率，仅需要计算出每个词或文本在给定词$w_t$下的条件概率： $$ p({w_t}|{w_1},{w_2},\u0026hellip;,{w_{t - 1}}) $$ 但是当文本过长时，模型参数空间实在是太大了，而且理论上来讲，大部分文章开头的词和中间的词也没啥相关性，导致原始模型在实际中并没有什么用。所以为了提高运算效率，增加可解释性，更多情况是使用当前文本的前n个文本来计算条件概率，也就是最常见的N-gram模型，它做了n-1阶的马尔可夫假设，认定词的出现就与其前n-1个词相关： $$ p({w_t}|{w_1},{w_2},\u0026hellip;,{w_{t - 1}}) \\approx p({w_t}|{w_{t - n + 1}},\u0026hellip;,{w_{t - 1}}) $$ 其中，当n=1时，也就是假设一个词的出现不依赖于其他任何词，模型被称为unigram模型，而n=2时，即假设一个词的出现仅仅依赖于其上一个词时，这个模型又称为bigram模型，同样的当n=3时，假设一个词的出现依赖于该词的前两个词，这时模型称为trigram模型，再同理，n=4,5\u0026hellip;不好意思，后面还真就没这么多了。\n为什么呢，主要有以下几个问题：\n由于模型复杂度和参数空间的指数型增长，当n\u0026gt;3时，参数空间已经非常稀疏，就别说处理更长程的上下文了，同时n从3到4的增加对性能提升并不明显，在tradeoff之后一般n不会超过3； 使用one-hot向量表征，向量长度对应于整个词表的长度，可能会面临维度灾难问题； 统计语言模型没有解决文本本身的表征问题，只是解决了文本之间的转移概率的问题； 当出现语料库没有的词汇时，句子的出现概率为0，不合理，需要引入平滑（smoothing）算法解决。 神经网络语言模型 由于Ngram等统计语言模型的问题，提出了NNLM模型和word embedding的概念。NNLM模型的基本思想可以概括如下：\n假定词表中的每一个word都对应着一个连续的特征向量； 假定一个连续平滑的概率模型，输入一段词向量的序列，可以输出这段序列的联合概率； 同时学习词向量的权重和Ngram概率模型里的参数。 NNLM模型的目标是找到一个合适的参数来拟合一个词序列的条件概率$p({w_t}|{w_1},{w_2},\u0026hellip;,{w_{t - 1}})$，其结构如下图所示，是一个3层的神经网络：\n第一层为输入层，将输入的N−1个one-hot词向量${w_{t - n + 1}},\u0026hellip;,{w_{t-1}}$，通过维度为$V \\times {\\rm{D}}$的矩阵C映射为分布式的词向量，其中V是词典的大小，D是Embedding向量的维度。这里的矩阵C就是要学习的词向量，因为n个one-hot表征的词向量输入到神经网络中，进行的操作是$Y = W\\times X$，等效于将n个词向量从Embedding层中提取出来的查表操作： $$ \\left( {\\begin{array}{c} 1\u0026amp;0\u0026amp;0\u0026amp;0\\ 0\u0026amp;1\u0026amp;0\u0026amp;0 \\end{array}} \\right)\\left( {\\begin{array}{c} {w_{11}}\u0026amp;{w_{12}}\u0026amp;{w_{13}}\\ {w_{21}}\u0026amp;{w_{22}}\u0026amp;{w_{23}}\\ {w_{31}}\u0026amp;{w_{32}}\u0026amp;{w_{33}}\\ {w_{41}}\u0026amp;{w_{42}}\u0026amp;{w_{43}} \\end{array}} \\right) = \\left( {\\begin{array}{c} {w_{11}}\u0026amp;{w_{12}}\u0026amp;{w_{13}}\\ {w_{21}}\u0026amp;{w_{22}}\u0026amp;{w_{23}} \\end{array}} \\right) $$ 第二层是隐藏层，组成就是简单的全连接网络和tanh激活函数；\n最后一层是输出层，也是一层全连接网络结合softmax，将Embedding层输出的N−1个词向量映射为一个长度为V的概率分布向量。\nNNLM通过引入连续词向量和平滑的概率模型，可以在连续空间内对序列概率进行建模，从而降低了数据稀疏性和维度灾难出现的概率，同时以条件概率$p({w_t}|context)$为学习目标去更新词向量的权重，也增加了可解释性。\n但仍然存在如下问题：\n模型使用的全连接神经网络，只能处理定长序列； 模型参数大小，与输入长度相关，长度太大时训练耗时太久。 Word2Vec模型 针对NNLM存在的第一个问题，Mikolov等人提出了一种RNNLM模型，用递归神经网络代替原始模型里的前向反馈神经网络，并将Embedding层与RNN里的隐藏层合并，解决了变长序列问题；对于第二个问题，将原始的NNLM模型的训练拆分为两步：1. 先训练出连续的词向量；2. 基于训练好的词向量去训练Ngram神经网络模型。而NNLM模型的计算瓶颈主要是在第二步的softmax，如果只是想得到word的词向量，可以对第二步的神经网络模型进行简化，Word2Vec模型就应运而生。\nWord2Vec模型主要有两个训练方案，两个加速手段，训练方案为Continues Bag-of-Words（CBoW）和Skip-gram，加速手段为hierarchical softmax（层次softmax）和Nagative Sampling（负采样）。\n任务举例 训练word2vec的目标并不是得到词的向量，词的向量其实只是学习到的“副产品”。真正训练的目标是：给定一个词，计算当这个词出现时，它前后的n个词出现的概率，下图是n=2窗口的场景：\nCBoW CBoW比较类似于NNLM，其思路就是利用上下文词预测中心词：输入中间词的前后共C个词，预测中间词，在这个过程中训练出词向量矩阵，表现为每个输入的多个上下文词（多个学生）从中心词标签（老师）那里获得知识。其模型结构如下图所示：\n其中：\n图中${x_{1k}},\u0026hellip;,{x_{Ck}}$ 表示第 k 个中心词的前后C个上下文的 one-hot 向量； 将 one-hot 向量输入存放词向量的矩阵${W_{V \\times N}}$进行查表，V为词表的大小，N为词向量的维度； 将查表得到的上下文向量直接进行求和，再通过一个$N \\times {\\rm{V}}$的矩阵${W\u0026rsquo;}$映射到输出层。 学习结束后，可以选择输入层到隐层的矩阵$W$或者隐层到输出层的矩阵${W\u0026rsquo;}$作为词向量矩阵，若选择后者，每一列就是对应单词embedding后的结果。\n以例子的结果，最终计算公式为：$P(fox|quick,brown,jumps,over)$，中间向量计算如下，其中C=2n： $$ {v_{mid}} = \\frac{1}{{\\rm{C}}}\\sum\\limits_{i = 1}^{\\rm{C}} {{W^T}{x^{(i)}}} = \\frac{1}{{\\rm{C}}}\\sum\\limits_{i = 1}^{\\rm{C}} {{v_i}} $$ 从隐层到输出层与另一个矩阵${W\u0026rsquo;}$相乘，若中心词为第j个词，那么输出的向量为${t_j} = {v^T}{u_j}$，其中${u_j}$是${W\u0026rsquo;}$的第j列，得到的结果送入Softmax进行概率归一化： $$ p({w_j}|{w_1},\u0026hellip;,{w_C}) = {y_j} = \\frac{\\exp ({v_{mid}^T}{u_j})}{\\sum\\nolimits_{k = 1}^{v_{mid}} {\\exp ({v_{mid}^T}{u_k})} } $$ 最大化似然等价于最小化负的对数似然，上式取对数有： $$ \\log p({w_j}|{w_1},\u0026hellip;,{w_C}) = {v_{mid}^T}{u_j} - \\log \\sum\\nolimits_{k = 1}^V {\\exp (v_{mid}^T{u_k})} $$ 对比NNLM有以下三个不同点：\n替换NNLM模型中的隐藏层，为更简单的投影层（只对输入的向量做累加操作）； 直接将Embedding layer的查表之后累加求和（NNLM是将输出结果拼接）； 将下文单词纳入上、下文环境，真正考虑了Context（NNLM的输入严格来说为上文文本）。 Skip-gram CBoW模型是从上下文到中心词预测中学习到词向量的表达，反之亦可，这就是利用中心词预测上下文词的Skip-gram模型，表现为每个输入的中心词（学生）从多个上下文词标签（老师）那里获得知识。模型结构与CBoW几乎反转，有输入层、投影层（其实是多余的恒等投影，加上是便与CBoW模型对比）和输出层：\n以例子的结果，计算公式为：$P(quick,brown,jumps,over|fox) = P(quick|fox) \\cdot P(brown|fox) \\cdot P(jumps|fox) \\cdot P(over|fox)$\nSkip-gram模型的预测概率过程用极大似然估计表示为： $$ \\prod\\limits_{t = 1}^T {\\prod\\limits_{ - n \\le j \\le n,j \\ne 0} {P({w^{(t + j)}}|{w^{(t)}})} } $$ 等价最小化对数似然函数： $$\n\\sum\\limits_{t = 1}^T {\\sum\\limits_{ - n \\le j \\le n,j \\ne 0} {\\log } } P({w^{(t + j)}}|{w^{(t)}}) $$ 最后Softmax概率归一化之后： $$ P({w_j}|{w_i}) = \\frac{{\\exp (v_i^T{u_j})}}{{\\sum\\nolimits_{k = 1}^V {\\exp (v_i^T{u_k})} }} $$ 其中，${v_i}$是来自矩阵$W$的词向量，${u_j}$是来自矩阵${W\u0026rsquo;}$的向量。 上式取对数可得： $$ \\log p({w_j}|{w_i}) = v_i^T{u_j} - \\log \\sum\\nolimits_{k = 1}^V {\\exp (v_i^T{u_k})} $$ Skip-gram模型的本质是计算输入word的input vector与目标word的output vector之间的余弦相似度，并进行softmax归一化。\n模型对比 CBOW训练速度更快。当窗口大小为2时，CBOW需要计算4个one-hot向量乘W矩阵计算均值并反向传播，会更新 Context(w) 的词向量，计算量为1次前向传播和1次反向传播，而skip-gram需要分别计算4个词的前向和反向传播； Skip-gram对于低频词（小数据集场景下）更加准确。CBOW通过学习上下文来预测这个词，相当于是完形填空，旨在预测最常见或者说概率最大的词汇，而skip-gram为了预测上下文，对训练数据的利用更高效（构造的数据集多），可以学习到更多低频词信息。 Hierarchical Softmax试图用词频建立一棵哈夫曼树，那么经常出现的词路径会比较短。树的叶子节点表示词，共词典大小多个，而非叶子结点是模型的参数，比词典个数少一个。要预测的词，转化成预测从根节点到该词所在叶子节点的路径，是多个二分类问题。本质是把 N 分类问题变成 log(N) 次二分类； Negative Sampling把原来的 Softmax 多分类问题，直接转化成一个正例和多个负例的二分类问题。 Word2Vec提速方案 W2V的计算瓶颈主要在Softmax上面，每计算一个词的概率都要对词典里的V个词计算相似度，然后进行归一化，消耗时间巨大，因此提出了两个提速方案：层次Softmax和负采样。\n层次Softmax Hierarchical Softmax方案是通过构造一个Huffman树，将归一化概率问题转化为一系列二分类的条件概率相乘。构造的Huffman树以词表为根结点，词为叶节点，中间节点仅仅是辅助作用，没什么实际的含义。叶节点的权值就是词频，带权路径长度指的是词频乘以路径的大小，带权路径最小这一条件使得构造出来的霍夫曼树中，高频词离根结点更近，而低频词离根结点更远。如下所示： 层次Softmax通过构造一颗二叉树，将目标概率的计算复杂度从最初的O(V)降低到了O(logV)的量级。但一个word出现的条件概率的变化，会影响到其路径上所有非叶节点的概率变化，间接地对其他word出现的条件概率带来不同程度的影响，而且当遇到生僻词的时候，在哈夫曼树中也要走很久，计算量也很大。\n负采样 将多分类训练方法转化为二分类，避免了Softmax分母求和计算，也就是给定$w_c$和$w_i$两个词，$w_i$在$w_c$窗口内的概率。正样本就是中心词$w_c$和窗口范围内每个词$w_i$组成的词对，负样本是中心词$w_c$和词库中随机抽取的k个词，k一般是5到20。\n负样本选择方案：\n完全随机取样：可能取到停用词或者高频无意义词汇； 每个词概率均等采样：取到的词对于文本可能没有代表性； 带权采样：$P({w_i}) = \\frac{f{(w_i)^{3/4}}}{\\sum\\nolimits_{j = 0}^n {(f{(w_j)^{3/4}})}}$，其中${f({w_i})}$是${w_i}$在文中出现的次数，3/4次幂处理是经验值，一定程度上提升低频词的权重。 具体执行方法是将所有的词按频率映射到0-1区间，随后按照特定距离等切分线段，映射到原有区间，当采样时，随机生成等距区间的随机数，即为负样本，若采样到本体，跳过。最后仅需要从采样结果中计算归一化概率分布，从而大大简化计算过程。\nword2vec的局限性 在模型训练的过程中仅仅考虑context中的局部语料，没有考虑到全局信息； 对于中文而言，分词会严重影响词向量的质量，因此，从某些方面来说word2vec对中文不是那么友好； 词向量模型的embedding矩阵在训练完成后便已经是固定了的，对于同一个词，在任意一个句子和环境下的词向量都是固定的，无法解决歧义等问题。 W2V代码细节 sigmoid近似计算：缓存预先计算好的sigmoid在区间[-6, 6]之间的值，小于-6为0，大于6为1，避免指数级别计算影响。\n低频词处理：建立词典时通常会设置一个min_count的阈值参数，剔除出现次数少于min_count次的词。\n高频词的处理：通常认为高频词（的、是等副词）往往只提供较少的信息，而且这些词对应的词向量在众多样本的训练过程中也不会发生明显的变化。因此对出现频率高于特定值t的词，进行下采样，按$$1 - \\sqrt {\\frac{t}{{f(w)}}}$$概率进行抛弃，f(w)表示该词在文本出现的占比，一方面可以提高的训练速度，另一方面也可以提升低频词的表示精度。\nFastText Facebook提出的类似于CBOW结构的快速词向量计算和文本分类工具。\n模型结构 输入层：词和子词（subword）的n-gram的特征向量；\n隐藏层：所有词的向量叠加求平均之后经过线性变换到隐藏层；\n输出层：Hierarchical Softmax输出文本类别。\n模型特点 字符级的n-gram：除了每个单词的词向量外，还为每个单词的n-gram字符添加一个向量，作为额外的特征，对于低频词生成的词向量效果会更好。因为它们的n-gram可以和其它词共享；对于训练词库之外的单词，仍然可以构建它们的词向量。我们可以叠加它们的字符级n-gram向量； 分层softmax：利用哈夫曼树构建，根据目标类别的多少自上而下构建，数目越多的类越在顶部； 各种提速的trick，如提前算好exp的取值之类； 核心思想：将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。 与CBOW的异同点 相似：输入都是多个经向量表示的单词，输出都是一个特定的target，隐含层都是对多个词向量的叠加平均。 不同：CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档；CBOW的输入单词被onehot编码过，fastText的输入特征是被embedding过；CBOW的输出是目标词汇，fastText的输出是文档对应的类标。 GloVe GloVe(Globel Vectors)算法，其实就是SVD分解与Word2Vec的结合。\n统计共现矩阵 在介绍GloVe的思想之前，先定义一个共现矩阵X，该矩阵中的${X_{ij}}$表示第j个单词出现在以第i个单词为中心，长度为n的窗口中的次数。将长度为n的窗口遍历整个语料库，则得到了共现矩阵X。\n对于大型语料库，我们可以认为统计的词共现矩阵X可以很好的描述词与词之间的相关性。但是，就像之前我们说过的，这样大的一个共现矩阵在实际使用中将会面临复杂的维度灾难问题，因此需要想办法词向量进行降维，比如之前的SVD对词-文档共现矩阵进行降维就是这样一种思想。\n对于word2vec，我们每次训练词向量，都是针对于局部语料进行预测，这就使得模型的训练过程中是很难考虑到整个语料库的全局信息的。我们通常将其称为一种预测模型，其目标是不断提高对其他词的预测能力，即减小预测损失，从而得到词向量。既能通过训练的方式得到固定维度的词向量表征，又能够使得到的词向量能够充分考虑到语料库的全局特征？\n简单来说，Glove相对于Word2Vec，需要提前统计词共现矩阵，并将其整合到代价函数之中，使得训练结果对于该统计是有一定的重建能力的。我们将其称为一种统计模型，其目标是优化减小重建损失，即降维之后的向量能尽量表达原始向量的完整信息。\nGloVe与Word2Vec的区别就在于GloVe考虑到了文本的全局特征，直观上来说比Word2Vec更合理。在语料库足够大的情况下GloVe的效果通常会更好一些，可以尝试的做法是将GloVe和Word2Vec进行拼接等操作。\n与其他词向量对比 词向量 不同点 Glove 利用全局信息，收敛更快，需要统计固定语料信息，滑动窗口构建共现矩阵，统计全部语料在固定窗口内词共现频次，没有直接利用共现矩阵，通过词频特性，将词向量和词频联系起来，损失函数最小平方损失函数 Word2Vec 局部语料库训练，特征提取基于滑窗，损失函数带权重的交叉熵，可以进行在线学习 fastText 考虑subword，可有监督学习文本分类，利用了层次softmax进行加速，引入字符级n-gram LSA 基于全局语料采用SVD进行矩阵分解 ELMOs 动态词向量，解决了上述静态词向量不考虑词序，不能识别的一词多义问题 各种词向量 特点 bag-of-words（one-hot、tf-idf、textrank）：维度灾难、语义鸿沟；\n矩阵分解（LSA）：利用全局语料特征，但SVD求解计算复杂度大；\n基于NNLM/RNNLM的词向量：词向量为副产物，存在效率不高等问题；\nword2vec、fastText：优化效率高，但是基于局部语料；\nglove：基于全局预料，结合了LSA和word2vec的优点；\nelmo、GPT、bert：动态词向量，可以随上下文变动；\nQuiz 衡量词向量的方式 词向量的“similarity”跟通常意义的近义词或相似词有本质上的区别，词向量更多的含义是“同位词”，即上下文相近的词。\n把词向量作为下游任务的输入，如NER，分类等任务； 相似度度量，数据集的余弦相似度应该更高； 词汇类比任务，king – queen = man - woman； 聚类任务，词聚类观察是否准确。 Word2vec得到的词向量为什么能够表征词语之间的语义近似关系 word2vec得出的词向量其实就是训练后的一个神经网络的隐层的权重矩阵，经过模型训练后，词义相近的词语就会获得更为接近的权重，因此可以用向量的距离来衡量词的相似度。\n词向量平均做分类的优劣 优势：分类模型简单，速度快，训练参数量少，在语句少的场景下，效果好；\n劣势：在长语句场景下，效果较差，词越多，信息量越杂，简单的做平均的话，重要的词信息会在平均的过程中被削弱。\n字向量和词向量 字向量可以解决未登录词的问题，避免分词造成的影响；\n词向量包含的语义空间更大，更加丰富，如果语料足够的情况下，词向量可以学到更多的语义信息。\n词向量如何做优化 引入attention机制增强敏感词重要词的权限，或使用self-attention重新调整语句词的分布，提高模型的学习能力，或者直接使用预训练模型bert来做学习。\nWord2vec 和 TF-IDF 在计算相似度时的区别 前者是稠密向量，后者是稀疏向量； 前者维度低很多，计算更快； 前者可以表达一定程度的语义信息，后者不行； 前者可以通过计算余弦相似度计算两个向量的相似度，后者不行。 Word2vec是否需要加入正则 正则化用来避免过拟合的问题，word2vec训练任务不存在过拟合问题，反而需要尽量高的拟合度以有效表达语义。\n参考 【深度学习】Word2Vec | 细语呢喃 (hrwhisper.me) NLP经典模型系列（一）：Word2Vec - 知乎 (zhihu.com) ","permalink":"https://boringleric.github.io/posts/tech/transformers_embedding/","summary":"词向量技术的发展趋势，从W2V到动态词向量。","title":"Word Embedding的简单了解"},{"content":"众所周知，要介绍Transformers，就得介绍Attention，要介绍Attention，就得介绍RNN，所以，先从RNN开始吧。\nRNN Recurrent neural network (RNN) 是经典人工智能神经网络之一，常用于处理时序数据相关任务，如NLP或者股票数据等。全连接网络（MLP）在处理这些任务上存在一些问题：\n对于不同的输入样本，输入和输出可能有不同的长度，输入层和输出层的神经元数量无法固定； 从输入文本的不同位置学到的同一特征无法共享； 模型中的参数太多，计算量太大。 RNN对上述问题是一个相对理想的解决方案，它扫描数据输入的方式，使得每一个时间步对所有网络参数是共享的，且每个时间步不仅会接收当前时刻的输入，同时会接收上一个时刻的输出，从而可以利用过去输入的信息来辅助当前时刻的判断。\nRNN网络的结构图如下所示，借用吴恩达老师的deep learning图片：\n其中可见两个输入和两个输出，输入$x^{}$表示当前时刻的输入样本，$a^{}$表示上一个节点激活函数的输出；而输出$\\hat y^{}$是当前时刻的输出，$a^{}$则是下一个时间步的输入。因此，RNN实际上在t时刻得到的结果是基于前t个输入的，最原始的RNN是单向的，只能够利用前面的信息，但是有时候单向的RNN很难解决问题，因此衍生了双向RNN。\nBPTT back-propagation through time（BPTT）算法是常用的训练RNN的方法，本质还是BP算法，由于RNN处理时间序列数据，所以基于时间反向传播。BPTT的中心思想和BP算法相同，沿着需要优化的参数的负梯度方向不断寻找更优的点直至收敛。\n截断BPTT是完全BPTT的近似，这是长序列的首选，因为完整BPTT的每个参数更新的前向/后向代价在许多时间步骤中变得非常高。缺点是，由于截断，梯度只能返回到目前为止，因此网络无法学习与完整BPTT一样长的依赖关系。\nRNN存在的问题 RNN虽然在处理时序数据上有一定优势，但其依然存在一些问题：\nRNN数据处理方式难以并行化，训练速度慢； 在数据较长的情况下，RNN容易出现梯度消失或梯度爆炸的问题。 对于梯度问题的处理，首先需要明确一个问题，RNN和DNN的梯度消失和爆炸的意义并不完全相同。DNN的梯度爆炸消失的本质是由于梯度反向传播造成的连乘效应，当网络太深，网络权值更新不稳定时造成；而RNN的特殊性在于，它的权重是共享的，当距离长了，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。\n那么要解决这个问题，有一种简单但是实用的方案就是梯度裁剪：若梯度大于某个阈值或者小于阈值的负数，就用直接让它等于阈值。另一种方案就是RNN的改进型LSTM和GRU，如下图所示：\nLSTM Long Short Term Memory（LSTM）网络，顾名思义就是为了解决RNN的长序列依赖问题，它设计了三个门结构来提供了信息选择性传输的能力。\n结构 LSTM的整体结构如下图所示，在每个时间步接收上个时间步的输入有两个，传给下一个时间步的输出也有两个。${x_t}$表示本次输入，$C_{t-1}$表示上一个单元模块的长期记忆输出，$h_{t-1}$表示上一个单元的隐藏状态输出，$C_t$表示本次的记忆输出，$h_t$表示本次的隐藏状态输出：\n下面会对这几个门结构进行拆分和展示。\n遗忘门：有两个输入，当前时间步的输入${x_t}$以及上一层输出的隐藏状态$h_{t-1}$，遗忘门通过这两个输入训练出一个门函数，注意这个门函数的输出是在(0,1)之间的，可以有效的用于判断是保留还是遗忘信息（接近1的值表示保留，接近0的值表示遗忘），将其与上一层输出的全局信息$C_{t-1}$相乘，表示全局信息被选择部分遗忘。\n输入门：输入同遗忘门，将接收到的$h_{t-1}$和${x_t}$送入激活函数为tanh的小型神经网络，这一部分与传统RNN无异，就是将上一时刻得到的信息与该时刻得到的信息进行整合。将整合信息与门函数的输出相乘，相当于同样选择有保留的提取新信息，并将其直接加在全局信息中去。\r下面就是本轮时间步过后，记忆信息$C_t$输出。\n输出门：将新的记忆信息$C_t$通过tanh函数后与sigmoid函数的输出相乘，则可以得到该时刻全局信息对下一个cell影响的隐藏状态$h_t$。\nLSTM存在的问题 虽然LSTM的三个门结构已经提供了一定程度的历史信息保持能力，但是通过长时记忆单元，类似残差链接。但后来加了遗忘门，遗忘门介于0-1，梯度仍有可能消失；梯度仍可能爆炸，但LSTM机制复杂，多了一层激活函数sigmoid，可以通过正则与裁剪解决。\n同时，LSTM并没有改变输入方式，时序性的结构依然很难具备高效的并行计算能力，当前状态的计算不仅要依赖当前的输入，还要依赖上一个状态的输出，另一方面，RNN模型总体上更类似于一个马尔可夫决策过程，较难以提取全局信息。\nGRU GRU可以看作一个LSTM的简化版本，将$C_t$与$h_t$整合在一起，并将遗忘门和输入门整合为更新门，输出门变为重置门，一定程度上降低了复杂度，收敛速度有所提升。结构如下图所示：\nCNN CNN同样适用于NLP任务中的特征提取，非常类似于基于N-gram的局部编码，但其使用场景与RNN略有不同。\n使用残差网络，解决梯度消失问题，解决梯度消失问题的本质是能够加速信息流动，使简单的信息传输可以有更简单的路径，从而使得网络做深的同时，能够保证良好的性能。 CNN提取长距离特征的能力收到其卷积核感受野的限制，实验证明，增大卷积核的尺寸，增加网络深度，可以增加CNN的长距离特征捕获能力。 为了防止文本中的位置信息丢失，NLP领域里的CNN的发展趋势是抛弃Pooling层，靠全卷积层来叠加网络深度，并且在输入部分加入位置编码，人工将单词的位置特征加入到对应的词向量中，位置编码的方式可以采用Attention的方案。 CNN和RNN要建立输入序列之间的长距离依赖关系，一般可以使用以下两种方法：1. 增加网络的层数，通过一个深层网络来获取远距离的信息交互；2. 是使用全连接网络，但全连接网络无法处理变长序列。 Seq2Seq and Encoder-Decoder Sequence to Sequence（seq2seq）技术突破了传统的固定大小输入问题框架，可以将神经网络运用于在翻译，摘要和问答相关任务上面。更强调解决问题的目的。\nEncoder-Decoder 结构主要是 NLP 领域里的概念。它并不特值某种具体的算法，而是一类算法的统称。Encoder-Decoder 算是一个通用的框架，在这个框架下可以使用不同的算法来解决不同的任务。更强调解决问题的方法。\nEncoder 又称作编码器。它可以将输入信息编码为语义编码C； Decoder 又称作解码器，它将语义编码C转换为所要的解决方案； Encoder和Decoder具体使用什么模型，都可以自己选择。通常有CNN，RNN，GRU，LSTM。 整个结构的示意图如下所示：\n由于Encoder-Decoder模型在编码和解码阶段始终由一个不变的语义向量C来联系着，这也造成了如下一些问题：\n所有的输入单词 X 对生成的所有目标单词 Y 的影响力是相同的； 编码器要将整个序列的信息压缩进一个固定长度的向量中去，使得语义向量无法完全表示整个序列的信息； 最开始输入的序列容易被后输入的序列给覆盖掉，会丢失许多细节信息，这点在长序列上表现的尤为明显。 Attention Attention机制的诞生就是为了解决上述encoder-decoder框架存在的问题的。Attention可以在上下文向量和输入之间创立一个关联，这些关联的权重就是专门为输出元素定义的。上下文向量包含了三种信息：1、编码器的隐状态；2、解码器的隐状态；3、源词汇和目标词汇的对齐。\nAttention机制的作用就是为模型增添了注意力功能，使其倾向于根据需要来选择句子中更重要的部分。加入Attention机制的Seq2Seq模型框架如下图所示：\n可以看出，和之前的Encoder-Decoder结构对比，带有注意力机制语义向量C变成了每一次预测都有不同的上下文信息的语义向量$C_i$。而生成单词$y_i$时，使用的也是各自的语义向量$C_i$：${y_i} = f({C_i},{y_1}, \\cdots ,{y_{i - 1}})$。\n当前时刻的中间语义向量$C_i$为输入信息注意力加权求和之后得到的向量，即： $$ {c_i} = \\sum\\nolimits_{j = 1}^{T_x} {a_{ij}{h(x_j)}} $$ 以Encoder，Decoder均为RNN为例，其中$T_x$表示输入文本的长度，$h(x_j)$为Encoder端的第j个词输入后生成的隐向量，$a_{ij}$表示Decoder端的第i个词对Encoder端的第j个词的注意力大小，即输入的第j个词对生成的第i个词的影响程度，生成$c_i$最关键的部分就是注意力权重$a_{ij}$的计算。\n$a_{ij}$的计算是通过一个对齐函数F来进行计算，对齐函数一般有两类，一类是点积；另一类是MLP网络，输入节点j和输出节点i，经过softmax就可以得到注意力分配的概率。\n生成目标单词$y_i$的计算概率为：$p({y_i}|({y_1},\u0026hellip;,{y_{i - 1}}),{x_i}) = g({y_{i - 1}},{s_i},{c_i})$，其中$y_i$为当前目标词的ground truth，$y_{i - 1}$是上一个节点的输出单词，$s_i$是当前节点的隐藏状态，$c_i$是生成当前词汇所需的语义向量，g为全连接层函数。\n当前Decoder的隐层状态$s_i$由上一层的隐藏状态$s_{i-1}$，输出单词$y_{i-1}$，语义向量$c_i$：${s_i} = f({s_{i - 1}},{y_{i - 1}},{c_i})$。\n语义向量$c_i$由注意力权重$a_{ij}$和Encoder的输出$h(x_j)$加权求和构成：${c_i} = \\sum\\nolimits_{j = 1}^{T_x} {a_{ij}{h(x_j)}}$。\n注意力权重$a_{ij}$由${y_i}$受到$x_j$的注意力分配$e_{ij}$得到：${a_{ij}} = \\frac{\\exp (e_{ij})}{\\sum\\nolimits_{k = 1}^{T_x}e_{ik}}$。\n注意力分配$e_{ij}$由encoder的输出$h(x_j)$和decoder的上一层隐状态$s_{i-1}$计算而成：$e_{ij} = F(s_{i - 1},{h(x_j)})$\n以NMT的global attention为例，对齐函数有下面三种实现方式： $$ score(h_t,\\bar h_s) = \\left{ \\begin{array}{c} {h_t^T{\\bar h}_s}\\ {h_t^T{W_a}{\\bar h}_s}\\ {v_a^T\\tanh ({W_a}[h_t^T;{\\bar h}_s])} \\end{array} \\right.\\begin{array}{c} {dot}\\ {general}\\ {concat} \\end{array} $$ 其中${h_t}$表示当前Decoder的隐状态，${\\bar h_s}$表示s时刻Encoder的隐状态。dot的好处就是计算简单，不引入额外参数，但是两个矩阵必须size相等；另外两类引入Trainable的矩阵，不需要qv向量size一致，但是增加了计算代价。\n机制 Attention机制的实质其实就是一个寻址（addressing）的过程，给定一个和任务相关的查询Query向量q，通过计算与Key的注意力分布并附加在Value上，从而计算Attention Value，这个过程实际上是Attention机制缓解神经网络模型复杂度的体现：不需要将所有的N个输入信息都输入到神经网络进行计算，只需要从X中选择一些和任务相关的信息输入给神经网络。\n将Source中的构成元素想象成是由一系列的(Key,Value)数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。 从本质上理解，Attention是从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。\n注意力机制可以分为三步：一是信息输入；二是计算注意力分布；三是根据注意力分布来计算输入信息的加权平均。\n分类 注意力机制的目的是将计算资源分配到更重要的任务。\n根据Attention的计算区域由下面几种类型：\nhard attention：注意力是只关注到某一个位置的信息，通过随机采样或最大采样来选取特征信息。但是其缺点是损失函数与注意力分布之间的函数关系不可导，因此无法使用在反向传播算法进行训练。一般使用软性注意力来代替硬性注意力。硬性注意力需要通过强化学习来进行训练。 soft attention/global Attention：全局计算，对所有key求权重概率，每个key都有一个对应的权重，这种方式参考了所有key的内容，通过加权求和的方式选取特征信息，计算量可能会比较大一些。 local attention：以上两种方式的折中，对一个窗口区域进行计算。先用Hard方式定位到某个地方，以这个点为中心可以得到一个窗口区域，在这个小区域内用Soft方式来算Attention。 根据所用文本信息分类：\ngeneral attention：使用除原文之外的额外信息，常用于需要构建两段文本关系的任务，query一般包含了额外信息，根据外部query对原文进行对齐，如阅读理解任务构造问题和文章的关联。 self attention：只使用原文本身的信息，key、query和value只和输入原文相关，寻找原文内部关系。 根据结构层次分类：\n单层attention：一个query对原文做一次attention； 多层attention：每一层在上一层基础上做attention； 多头attention：使用多个query对原文做多次attention，每个query关注不同部分，相当于重复做多次单层attention，最后将结果拼接。 self attention的一些理解 self attention的另一种思考方式可以参考推荐系统，query和key的点积类似推荐系统两个特征向量之间的点积，可以得到特征对于最终结果的贡献度得分。落到文本上面，对句子本身进行注意力权值计算，使其更能够把握句子中词与词之前的关系，从而提取出句子中的句法特征或语义特征。\n另一种思考方式类似检索系统，数据库中的文档是value，文档的索引是key，而输入的数据作为query，attention就是获得query和key的匹配程度加权。\n下图是一个self-attention的例子，输入$x_2$对于其他输入做self-attention得到$y_2$输出：\nAttention 和 CNN 的异同点 相似点：无论是attention机制还是CNN中的卷积，都是以加权求和的模式对数据进行加工。在Transformer中，Multi-Head Attention在后面还有一个特征融合过程，与CNN中逐通道卷积最后沿着通道求和做特征融合比较相似。甚至可以认为CNN中的卷积是在全图范围内将全部的注意力用于当前卷积窗口。 不同点：（1）attention的注意力权重是动态的按照输入计算出来的，但是卷积一但训练完毕就和输入无关；attention考虑的是在全输入下找出重点，而卷积更多的是窗口操作模式。（2）在Transformer中有明确的Q、K和V的概念，在卷积中没有。 参考 图文介绍RNN注意力机制 Attention用于NLP的一些小结 Transformers from scratch ","permalink":"https://boringleric.github.io/posts/tech/rnn_attention/","summary":"简单介绍RNN到Attention技术的发展过程。","title":"从RNN到Attention的简介"},{"content":"使用self-attention的经典模型Transformer，和基于其Encoder结构的预训练模型Bert，为NLP领域开创了新时代。\nTransformer Transformer是在Attention is All You Need提出的的模型框架，用于解决序列转写任务。这里借用经典的图做一下解释：\nTransformer遵循Encoder-Decoder结构，上图左侧Inputs部分为Encoder，右侧Outputs部分为Decoder，两者均为多个Transformer Block堆叠而成，但Encoder的Transformer与Decoder的Transformer是有略微区别的：\nEncoder端使用的attention不需要masked，而Decoder需要Masked； Decoder中多了一层Encoder-Decoder attention，其中的Query是self-attention 计算出的上一时间i处的编码值，Key和Value都是Encoder的输出。 Embedding Embedding是transformer的基础，文本需要转换为embedding，才可送入Encoder做后续运算。Transformer的Embedding包括两个部分，一个是Word embedding，另一个是Positional embedding。\nWord Embedding 与传统Word Embedding技术一致，通过查表方式，根据token的id将token转为embedding。\nPositional Embedding 由于Attention采用的并行计算，导致token的计算逻辑，不具备任何的位置信息，次序打乱之后也不会影响attention的计算结果，所以必须引入位置向量，确保位置信息不会被丢弃。在Transformers中，位置信息使用的绝对位置，构造位置编码的公式如下所示： $$ \\left{ {\\begin{array}{*{20}{c}} {P{E_{2i}}(p) = \\sin (p/{{10000}^{2i/{d_{pos}}}})}\\ {P{E_{2i + 1}}(p) = \\cos (p/{{10000}^{2i/{d_{pos}}}})} \\end{array}} \\right. $$\n对不同维度使用不同频率的正/余弦公式进而生成不同位置的高维位置编码，利用正余弦函数实现相对位置信息的表示。\n为什么奇偶维度之间需要作出区分，分别使用 sin 和 cos 呢？奇偶区分可以通过全连接层帮助重排坐标，所以可以直接简单地分为两段(前 256 维使用 sin，后 256 维使用 cos)。\n组合方法 词向量和位置向量二者直接对应相加，得到最终的向量嵌入。\n这样操作是否会影响到原始的输入特征呢？根据johnson–lindenstrauss 定理：通俗的理解是两个高维的向量总是近似正交的，可以认为无论是add还是concat，两个向量耦合性小，丢失的信息量少，因为维度足够高，所以即使有部分信息被正余弦信号遮蔽，仍然有足够的特征可以用来辨识。\nEncoder 从Encoder段开始介绍，由上图可知，一个Encoder块主要由两部分组成：多头注意力机制(Multi-Head Attention)和前馈神经网络(Feed Forward)，二者都通过残差结构连接。\n多头注意力机制 多头注意力模块结构如下图所示：\n这里可以看出整个模块是由h个Scaled Dot-Product Attention模块堆叠而成的，可以类比CNN中同时使用多个卷积核的作用，有助于捕捉到更丰富的特征信息。主要步骤如下：\n向量生成：将token的embedding经过线性变换，生成Q、K、V三个向量； Attention切分：将Attention按照指定的head数量切分，如512维度分成8个head，每个head维度为64； Self Attention计算：对每个head进行Scaled Self Attention计算； 连接：对于h个Scaled Self Attention的输出，进行简单的拼接，并通过与一个线性映射矩阵${W_o}$与其相乘（目的是对输出矩阵进行压缩），从而得到整个Multi-Head Attention的输出。 下图是一个简单的multi-head attention计算过程：\n输入句子 Thinking Machines; 将原句Tokenizer后转成Word Embedding X，假设维度512； 假设头的个数为8个，将Embedding X切分为8份，每份维度64，并将其分别赋予key、query、value，每个都乘以对应权重矩阵$w_i^k$，$w_i^q$，$w_i^v$，得到输入向量$K_i$，$Q_i$，$V_i$； 8个头分别计算Attention矩阵，${Z_i} = soft\\max (\\frac{{{Q_i}K_i^T}}{\\sqrt {{d_k}}}){V_i}$； concat拼接融合8个头的结果${Z_i}$，点乘权重$W_o$得到最终输出Z。 在Multi-Head Attention中，最关键的部分就是Self-Attention部分了，也就是上图所说的Scaled Dot-Product Attention，计算过程可以用以下的矩阵形式进行并行计算: $$ Attention(Q,K,V) = softmax (\\frac{{Q{K^T}}}{{\\sqrt {{d_k}} }})V $$ 其中，Q, V, K分别表示输入句子的Queries, Keys, Values矩阵，矩阵的每一行为每一个词对应的向量Query, Key, Value向量，$d_k$表示向量长度。也正是因此，Transformer具有RNN所缺失的并行计算能力。\n依旧以Thinking Machines为例，下图给出了两词计算self-attention计算的过程：\n首先对每个输入单词向量Embedding X生成三个对应的向量：Query，Key 和 Value。这三个向量相比于向量X要小的多，因为每一个都会被拆分成headnum个独自计算，并且最后这headnum个输出会拼接恢复成原维度向量； 用Queries和Keys的点积计算所有单词相对于当前词(图中为Thinking)的权重得分，该分数决定在编码单词Thinking时其他单词给予的权重贡献度调整； 将Score除以向量维度64的平方根，再对其进行Softmax，将所有单词的分数进行归一化，这样对于每个单词都会获得所有单词对该单词编码的贡献分数，当然当前单词将获得最大分数，但也将会关注其他单词的贡献大小； 对得到的Softmax分数乘以每一个对应的Value向量； 对所得的所有加权向量求和，即得到Self-Attention对于当前词Thinking的输出。 Tips：\nKQV是怎么从token转换而来的？下面这个图可以给出结论，对于每个向量，都会维护一个映射矩阵，并且不同Self-Attention模块之间的权值不共享，将输入的矩阵X与三个映射矩阵相乘，得到三个输入Queries、Keys和Values。要注意这里的X可以是初始输入的token(也就是Input Embedding + Positional Embedding)也可以是上一层Transformer的输出。 self-attention为什么要除以$\\sqrt {{d_k}}$？\n首先，点积的目的是为了获得query和key的相似度，后续会接softmax进行计算，当query和key之间的方差过大时，softmax计算结果会出现非0即1的情况，会向很小的梯度的方向靠拢，训练效果不佳。而除以$\\sqrt {{d_k}}$缩放了矩阵乘积范围，可以获得更加平滑的softmax结果，训练效果更好；\n假设Q和K是标准正态分布，均值为0，方差为1。它们的矩阵乘积将有均值为0，方差为 ${d_k}$ 。而$\\sqrt {{d_k}}$用于缩放时，Q和K的矩阵乘积的均值为0，方差为1，可以获得一个更平缓的softmax。\n$$ q \\cdot k = \\sum\\nolimits_{i = 1}^{{d_k}} {{q_i}{k_i}} $$\n为什么一定要定义kqv三个元素？\n注意力机制目的是要学习到一个权值，自注意力机制可以通过权重矩阵学习词与词之间的关系，但是如果只定义一个元素，无法分别表示自身与其他元素，其他元素与自身的权重，所以至少需要kq两个元素表示；而找到了权重之后，为了给原值加权，需要另外定义一层可以学习的参数，增强网络的学习能力。\n另外，如果只定义一个元素，那么很显然每个元素对于自己的注意力永远是最大的，挤压了其他的注意力得分，参数太少也影响模型容量。\n有解释是参考了Memory Network，用输入的query检索k-v memories，得到相似memory的key，计算相关性后对value进行加权求和。\n还有解释是对embedding矩阵起到降维，降低运算量的目的。\n前馈神经网络 Transformers的前馈网络层很简单，由两个全连接层构成，第一层的激活函数为 ReLu，第二层不使用激活函数。在Bert代码中，Intermediate Layer作为FFN的第一层全连接层，其维度是（hidden_size，intermediate_size），且intermediate_size比hidden_size大很多，Output Layer作为第二层，不使用激活函数。\n为什么要加入一层前馈神经网络呢？可以看出，多头Attention进行的主要是矩阵乘法，也就是说进行的全都是线性变换，也就是对权重的重新分配，而线性变换的学习能力不如非线性，尽管Attention机制学习到了每个词汇的表达，但是表达能力不强，需要通过激活函数强化表达能力（提供非线性映射，增强数据大的部分，抑制数据小的部分）。\n在论文中对这一部分的解释是，可以将其视作两个具有$1\\times1$卷积核的卷积神经网络，$1\\times1$卷积核可以让每个位置进行独立运算，不需要关注周边信息，而中间扩增的结构类似于Resnet的bottleneck结构，减少计算量，增加网络的表示能力。\nLayer Normalization 在每个Block中，无论是MHA还是FFN都添加了层间归一化模块，随着网络深度的增加，保证数据特征分布的稳定性，加入正则化，使用更大的学习率，加速模型的收敛速度。同时，正则化也有一定的抗过拟合作用，使训练过程更加平稳。\n为什么Transformer使用LayerNorm ，而不使用BatchNorm？BN的特点是强行拉平数据之间的分布，使得模型收敛速度更快，并且起到了正则化的作用，使模型效果更佳。但是，BatchNorm对Batch Size大小很敏感，并且在LSTM网络上效果极差。LayerNorm是横向归一化，不受Batch Size大小的影响，并且可以很好地应用在时序数据中，而且不需要额外的储存空间。《Rethinking Batch Normalization in Transformers》一文对比了LayerNorm和BatchNorm对于Transformer的作用，并且提出了一种新的归一化方式。\n残差模块(Residual Block) 残差模块是借鉴自CNN的思想，可以解决梯度消失和网络退化问题，构造更深的网络。 首先，残差模块能让训练变得更加简单，如果输入值和输出值的差值过小，那么可能梯度会过小，导致出现梯度小时的情况，残差网络的好处在于当残差为0时，改成神经元只是对前层进行一次线性堆叠，使得网络梯度不容易消失，性能不会下降。 其次，随着网络层数的增加，网络会发生退化现象：随着网络层数的增加训练集loss逐渐下降，然后趋于饱和，如果再增加网络深度的话，训练集loss反而会增大，注意这并不是过拟合，因为在过拟合中训练loss是一直减小的。 加入残差网络后，在前向传播时，输入信号可以从任意低层直接传播到高层。由于包含了一个天然的恒等映射，一定程度上可以解决网络退化问题。\nDecoder Transformer的Decoder端主要由三个模块构成，分别是Masked Multi-head attention、Encoder-decoder attention和Feed Forward Layer，除FFL外，另外两个Attention的结构与Encoder端的Attention结构也是一致的，只是在训练和预测阶段操作稍有不同。整体流程图如下所示：\nMasked Multi-head attention 这一个Attention采用了Mask操作，因为在解码过程中，需要顺序进行，得到$i$个词之后，才可以进行$i+1$个词的预测，通过Mask操作可以防止第$i$个词看到后续词的信息。\n需要注意，在Training过程中，需要Mask操作，而在Prediction过程中，无需Mask操作。\nDecoder训练过程中可以使用Teacher forcing和并行化训练，Mask操作如下所示，一般做法是将需要被mask的向量设置为负无穷，那么经过点积和softmax之后其结果趋近于0，对最终结果也几乎没有任何影响，只对QK操作有影响：\n为什么在训练阶段可以并行化训练，因为已经知道了正确的译文，因此可以同时模拟已知前$i$ 个译文token情况下，训练decoder去预测第 $i+1$ 个译文token。\nEncoder-decoder attention 这个部分的Attention的目的是建立Encoder和Decoder之间的关系，主要区别在与Attention的K，V矩阵不是来自上一个Decoder Block的输出，而是来自Encoder端的输出，但是Q矩阵还是来自于上一个Decoder Block的输出。\n输出端 在最后一层Decoder Block后，接Linear将输出转为vocab_size长度，后接softmax算出最大概率logit输出。\nBert 结构 Bert是一种语言模型，使用的是Transformers的Encoder模块，并在embedding层添加了segment embeddings做NSP任务。\n学习任务 MLM任务，主要是预测掩码元素，在原文中15%的token被掩掉，其中80%的token使用[mask]替换，10%的token使用随机token替换，10%的token保持不变。mask太少会导致学习不充分，增加训练时长；mask太多会导致使一段文本中丢失太多的语义信息。（跟CBOW相似，15%相当于7个token预测一个token，刚刚好。） NSP任务：预测下一句话，是否是前一句话的下一句话。这个任务主要使用CLS的Embedding，它能否表示sentense embedding呢？从任务可见，pretrain的cls embedding很大程度上就是在编码NSP任务所需的高阶特征，也就是描述两段文本是否构成上下文关系，所以说，这个特征既然不是描述sentence语义的，直接用到sentence embedding上效果不好，但是finetuning之后可以获得不错的效果。 BERT中的token mask方法对理解文本语义任务极有帮助，但这种方式具有硬伤： 预训练时输入句子带mask，而fine-tune不带，导致上下游任务不匹配问题。 transformer的encoder的all attention结构，需要句子all token参与attention，这注定与自回归的NLG任务无法配套使用。 分词问题 词表的构建选用基于BPE算法的WordPiece算法，将通过频率合并字符的频率变成了通过语言模型的似然进行合并，其思想是选择能够提升语言模型概率最大的相邻子词加入词表。\nBPE与Wordpiece都是首先初始化一个小词表，再根据一定准则将不同的子词合并。\nBPE与Wordpiece的最大区别在于，如何选择两个子词进行合并：BPE选择频数最高的相邻子词合并，而WordPiece选择能够提升语言模型概率最大的相邻子词加入词表。\nBasicTokenizer： 转unicode -\u0026gt; 去除空字符、替换字符、控制字符和空白字符等奇怪字符 -\u0026gt; 中文分词 -\u0026gt; 空格分词 -\u0026gt; 小写、去掉变音符号、标点分词；\nWordpieceTokenizer：大致分词思路是贪婪最长优先匹配算法，按照从左到右的顺序，将一个词拆分成多个子词，每个子词尽可能长。\n对中文：BERT 采取的是将每一个汉字都切开的策略。\nWhole-word-mask策略：WordPiece构建的词表，会有很多子词，掩码时会导致信息不全。引入全词掩码，若一个word被切开，则全部切分的子词都要被掩掉。\n可能的问题：\n词汇量不足：对于英文词汇，专有名词被拆分； 拼写错误/缩写问题：拼写错误/缩写可能会导致模型性能大幅下降，subword导致切分与原文完全不一致； 前缀问题：WordPiece 旨在处理后缀和简单的复合词，没有前缀的概念。 解决方法：\n超大规模预训练数据和模型结构； 纠错，预处理删除无意义字符串，缩写扩充； 分解复合词，将前缀视为复合词并将它们分开。 Quiz Bert模型的做句向量的缺陷：直接使用Bert做句向量的输出，会发现所有的句向量两两相似度都很高。因为对于句子来说，大多数的句子都是使用常见的词组成的，Bert的词向量空间是非凸的，大量的常见的词向量都是在0点附近，从而计算出的句子向量，都很相似。 如何解决Bert句向量的缺陷：使用双塔模型，将两个句子传入两个参数共享的Bert模型，将两个句向量做拼接，进行有监督的学习，从而调整Bert参数，此方法叫sentencebert；使用无监督或者有监督的对比学习，将同一个句子传入相同的bert(dropout = 0.3)得到标签为正例的一个句子对。通过这种方式来做Bert的微调整，得到SimCSE模型。 参考 The Illustrated Transformer Transformers Explained Visually 深入理解Transformer模型 ","permalink":"https://boringleric.github.io/posts/tech/vanilla_transformers/","summary":"Transformer结构的分别介绍，Encoder和Decoder的拆解。","title":"原生Transformer架构简介"},{"content":"A Boring Guy\n","permalink":"https://boringleric.github.io/about/","summary":"A Boring Guy","title":"About"},{"content":"上一部分我们了解了Rasa NLU处理用户输入的过程，接下来将介绍Rasa Core在接收到NLU处理的用户意图之后，如何进行下一步操作的。\nRasa Core 基础结构 首先来看一下Rasa Core的基础架构和信息流图：\n可以看出，Rasa Core主要包含的有如下几个大模块，它们各自作用不同：\nAgent：这个模块是Rasa核心功能的承载部分，其他模块初始化，模型存储与加载，客户信息输入输出的通道，都是Agent模块来负责的； MessageProcessor：这个模块是对输入信息处理的核心模块，Agent接收输入信息后的所有处理过程都是在这个模块进行的； Tracker：这个模块说白了，就是本轮对话的状态仓库，无论是NLU解析返回的内容，还是Action执行的内容，都需要存储到Tracker中； Policy：这个模块是策略相关模块，内置了三大类策略，根据Tracker的状态来预测下一个要执行的Action； Action：这个模块包罗万象，Rasa将一轮对话中可能遇到的所有条件都抽象成了Action来进行操作； NLG：这个模块顾名思义，就是产生给用户的回复信息，不过不要太寄予厚望，这个NLG功能非常简单，并不像想象中的智能。 接下来我们将会对上述几个模块分别进行介绍（建议结合代码看）。\nAgent模块 Agent模块为Rasa提供模型训练、信息处理、模型加载、action预测、输出通道处理功能。在代码中体现的就是很简单的两个主要功能：\n创建MessageProcessor、创建NLG、创建Tracker Store、创建Lock Store； 接收用户message，然后调用MessageProcessor去解析Message。 以及一些便于API调用的其他功能，如根据用户id或tracker预测下一步Action，执行一个action等内容，相对主流程来说不算重要，暂且不提。\nMessageProcessor模块 这个模块是Rasa Core的重头戏之一，它提供了用于对话机器人沟通的主接口，在代码中，重点是如下几个接口：\nhandle_message：万恶之源，所有的初始信息被送入此地，首先调用parse_message函数，并根据解析结果更新Tracker，如果有识别到的意图，则会对其进行槽位提取并填充，随后调用_run_prediction_loop开始进行内部状态循环，直到聆听状态； parse_message：消息处理的第一关，会调用NLU模块解析输入的消息，返回解析后的字典结构，包含消息的意图、包含的实体以及意图排序，比如说我们输入一个“hi”，系统会返回对应的识别结果：\r_run_prediction_loop：这个函数会循环判断是否需要预测下一个action以及是否需要对消息进行处理，在这里会调用predict_next_with_tracker_if_should来对当前状态进行预测，以获得下一个需要行动的Action，接下来会执行 _run_action函数来判断是否跳出本次循环； predict_next_with_tracker_if_should：这个函数首先判断是否达到了本次对话轮次数，若没有达到，会调用_predict_next_with_tracker进行预测； _predict_next_with_tracker：这个函数会去调用PolicyPrediction模块，获得预测结果，可见策略预测模块的RulePolicy预测出了最高置信度的Action应该是第22个index的Action，这里的index可以在导入的domain-action_names_or_texts中获得，也就是\u0026rsquo;utter_greet\u0026rsquo;这个action：\r_run_action：当上一步获得了要执行的action，在这一个函数中就可以执行对应Action的操作，包括BotResponse，Listen等等，而执行结束后也不会一走了之，需要继续对tracker进行执行状态的更新，并预测下一步要执行的action，继续3-6的循环，直至action被置为Listen状态为止。 除了上述几个主要函数，还有一些关于任务提示器reminders，通过输出channel将机器人回复推送给用户等功能，也都包含在这个模块中，可以按需查看源码。\nTracker模块 全称为DialogueStateTracker，顾名思义，一轮对话的状态记录仪，其功能也是关于各个状态增删查询：\n重点关注如下内容：\nevents：在Rasa中，任何和对话相关的操作都会被抽象成Event存于此处，可以看出，从对话Session启动开始，到用户输入，机器人回复，监听状态等一系列操作事件都被记录于此； latest_action：这里记录了最近一次执行的action； latest_bot_utterance：这里记录了最近一次机器人的回复； latest_message：这里记录了最近一次用户的输入信息，包含用户意图，会话id等内容； slots：这里给出需要填充的一系列槽位信息。 至于函数功能，主要就是一个update不断更新Events队列，其他的也是和查询相关的功能居多。\nPolicy模块 Policy模块负责对话策略管理，也是算法相关的重点模块，Rasa内置了3个对话策略，分别为：\n规则策略（rule_policy）：基于规则设置的策略，拥有最高的权重，依据手动配置的rules.yml来做判断，当满足条件时，对应action的置信度设为1，否则设为0； 记忆策略（memorization_policy）：这个策略是从stories.yml中学习而来，如果当前对话匹配stories中曾经配置过的对话信息，则会将对应的action置信度直接设为1，否则设为0。在这个配置中会有max_history超参数，代表story可被切分的对话轮次数； TED策略（ted_policy）：全称为基于Transformer嵌入的对话管理策略（Transformer Embedding Dialogue Policy），用于下一步Action预测和实体识别任务，NER任务依旧是通过Transformer+CRF完成，而Action预测通过将Transformer的输出映射到另一个embedding，同目标label的embedding计算余弦相似度获得。 可能的几个小问题： TED模型的训练数据是什么样子的？在Rasa中，会将Story抽成[user, prev_action]对，从0到max_history次数不等，而要预测的目标则是对应的action； 这些数据怎么转换成向量的？这些数据会被转换成intent/action_name长度的一维one-hot向量，随后通过scipy转换为稀疏特征向量； intent和action_name长度不一样的话，维度不一致，怎么做到一致的呢？答案是，分开处理，在这里，用户的输入信息会被送入单向用户信息处理Transformer，而之前的action信息会被送入单向机器人信息处理Transformer，最后将两个feature concat之后送入下一级Transformer； 这些特征之间是怎么映射起来的？通过StarSpace方法，将对话特征与action特征联合建模； 推理的时候怎么做的呢？计算所有action与对话特征之间的相似度，选择相似度最高的action返回。 UnexpecTEDIntentPolicy也是一种策略吗？看名字可以看出，这个策略使用的和TED是一类模型，但是其主要目的不在于预测策略，而在于处理用户输入信息不符合预期的情况，唯一触发action_unlikely_intent。 Action模块 Action是Rasa对用户信息处理的标准操作方法，Rasa内部定义了许多类型的Action，以供维护对话状态使用，其中内置的有如下Action：\naction_session_start：只在一通对话刚刚开始时设置，并重置对话Tracker，也支持重写该Action以在对话开始时发送消息给用户； action_listen：让机器人停止其他操作，等待用户输入； action_restart：接收到/restart指令时，重置本轮对话，清除历史记录； action_default_fallback：当开启兜底机制，并且所有的意图预测置信度都非常低的情况下会被触发，发送兜底消息给用户； action_deactivate_loop：跳出激活的active loop，重置槽位，当表格填充时出现意外（非预想场景）时会有这种情况； action_two_stage_fallback：在处理低置信度场景时可以启用，会不断向用户提出澄清意图的请求； action_default_ask_affirmation：当action_two_stage_fallback启用时，会向用户请求澄清意图，若用户确认时返回该Action； action_default_ask_rephrase：当action_two_stage_fallback启用时，会向用户请求澄清意图，若用户不确认时返回该Action； action_back：当输入/back指令时，撤销上一轮用户-机器人的对话； action_unlikely_intent：只会由UnexpecTEDIntentPolicy触发， action_extract_slots：针对表格填充使用，更新对话的槽位信息。 除此之外，对用户的各种回复信息则属于Responses系列action，以utter_作为开头，在domain.yml中的responses定义。\n最后，还有自定义action，继承rasa_sdk的Action类即可，你想要的，只要能写出来，都可以应写尽写，存放于actions文件夹，并在domain.yml定义，需要注意：自定义的action需要action server启动才会被调用到！\nNLG模块 依据对话状态产生机器人回复信息，一句random_response解君愁，它会很贴心的在你配置好的responses中随机挑选一句返回用户。\n总结 至此，我们一起学习了Rasa Core的结构，以及内部的一些算法，这样，Rasa最重要的两个部分，NLU和Core就大致看完了，下一节会对一些细节进行补充，并介绍部分3.x版本和2.x版本的不同点。\n","permalink":"https://boringleric.github.io/posts/tech/rasa3_core/","summary":"Rasa3 Core接受到NLU的意图后，如何进行推理并产生回复的。","title":"Rasa3 Core的处理逻辑简介"},{"content":"在这一部分，我们来一起实践一个比较复杂的基于Rasa 3.2.1版本的机器人。代码见此处，查天气功能借鉴了这里的代码，表示感谢！\n基本环境 python 3.8.12 rasa_sdk 3.2.2， 推荐使用Conda创建虚拟环境， 玩坏了也不怕。\nRasa初始化 切换到要创建Bot的文件夹，在对应Conda环境cmd输入rasa init即可创建，无需训练，建好的bot如下图所示：\n其中actions文件夹配置需要的第三方action功能，在这里我们会写歌曲搜索的一系列action；data文件夹提供nlu.yml、rules.yml、stories.yml；tests文件夹相对不重要，接下来将会从config开始一步步配置。\nRasa Config配置 打开config.yml，可以看到，有如下几个重点配置项：recipe，language，pipeline，policies。\n其中:\nrecipe的设计是与不同的组件和策略挂钩的，目前保持default.v1不动就可以了； language为这个bot设定主要语言，以两字表示，参照Spacy的语言表述形式，我们将en改为zh，即可改为中文； pipeline配置执行NLU的组件，必选：分词器、特征抽取器、分类器，可选：实体同义词映射、回复选择器、兜底分类器。由于这次要玩的花一点，所以： 特征抽取器选择LanguageModelFeaturizer，model_name选择bert，model_weights选择bert-base-chinese即可，这样系统会自动从huggingface的transformers库拖权重下来，如果有本地缓存也可以将model_weights设置为本地缓存路径； 分类器选用DIETClassifier； 虽然在Rasa的LMFeatureizer中，会对文本自动进行tokenizer，但是如果不配置tokenizer，系统会报错，NLU会把分词后的token放入返回的text_tokens中，所以还是需要一个分词器，可以使用jiebaTokenizer，因为切分结果对LanguageModelFeaturizer并不影响，觉得不合适的话也可以把transformers的tokenizer拆出来作为单独的部分，下面的实践是将tokenizer拆出来单独使用的； 兜底分类器选择threshold为0.3的FallbackClassifier，需要在rules中为nlu_fallback设置规则； 回复选择器设置100epoch的ResponseSelector； 添加EntitySynonymMapper，用于nlu的synonym处理； 添加RegexFeaturizer和RegexEntityExtractor用于nlu的lookup处理，需要注意，处理中文需要将RegexEntityExtractor的use_word_boundaries置为false； 至于其他featurizer，也可以添加作为sparse特征使用，这里就不添加了。 policies不用多说，我全都要。 因此，修改后的配置图为：\nRasa domain配置 domain.yml里面配置了大量的有价值信息，默认配置有intents、responses、session_config，显然不太够用，为了添加一些实体信息，我们还会引入entities、slots、forms和自定义的actions来完善domain的配置。\nintents：这里定义了可能需要解析的用户需求，如果用户需求用到了对应的实体，需要标注use_entities并在下面列出对应的实体名称：\rresponses：这里定义了各种系统回复及其对应的回复内容：\rentities：这里列举可能出现的实体名字； actions：这里列举自定义的各类action：\rforms：这里定义了需要完成填充的表格，表格内包含各类槽位，每个槽位都对应着实体，同时需要注意一点，rasa不会自动清除词槽，所以如果需要提交完表单后重置词槽，可以参考代码里的action_reset_all_slots，同时在rules配置即可：\rslots：这里定义了在表格中对应的槽位，以及每个槽位的属性，来源，实体等，注意，如果制定了intent，那么该槽位只接受来自于对应intent的实体，其他的intent就算识别出来实体也不会填充：\rsession_config: 定义了本次对话的过期时间，槽位继承相关，这里不做更改。 Rasa NLU配置 data文件夹下的nlu.yml最最最主要的功能是负责意图识别，所以里面包含的nlu，lookup，和synonym都是为意图识别服务的。\nnlu：nlu部分定义的就是意图，以及意图的表达形式，越全面越好：\rlookup：简言之这个就是一个查表功能，要结合config.yml的RegexFeaturizer和RegexEntityExtractor使用来提取实体：\rsynonym：这里专门配置同义词，也就是一个实体不同的说法，需要结合config.yml的EntitySynonymMapper使用，会将所有的表述统一改为指定的词：\rRasa Rules和Stories配置 rule和story有许多区别和坑，简单的概括一下：\nrule定义结束的action一定是后面会跟action_listen，所以配置了rule的intent-action对，在story里面不可以继续跟action，也就是不可以写intent-action-action，而没有在rule内配置的intent-action可以这么写； rule的命中评级高于story，有冲突的情况下优先执行rule； story是让TED Policy启发式学习的，因为流程多样，所以写法多样，但是最终学到的效果可能一般，比如，story里面定义了“姓名-性别-查天气”的问法，TED也学得很好，F1=1的评分，但是在执行中，用户问了“功能-查天气”，没有命中Rule，TED在预测的时候可能就懵了，默认给了action_listen，导致用户体验不好； 官方推荐少写rule多写story。 至于rule的写法，在不涉及到表格填充的时候很简单，就是：\nintent-action对，可以无限制写下去。但是一旦涉及到form的填充，问题就来了：\n可以看到，复杂程度大幅提高，以询问天气情况意图为例，在接受到这个意图之后，需要进行城市（city）和时间（date）的槽位填充，而这个槽位是定义在weather_form的，所以需要先激活weather_form的action，根据实体识别的结果填充对应槽位，当槽位填充完成之后，会进入active_loop，active_loop表示，表格内槽位完全填充完成之前，会一直停留在表格填充过程；而下面的condition表明，只要处在weather_form的填充中，就会一直关注，到什么时候为止呢，也就是表格填充完成-\u0026gt; active_loop为null时，才会执行下一个action，也就是action_weather_form。\n至于story的写法，更加多样化，只需要注意和rule的冲突就可以了：\nRasa 其他配置 credentials.yml：配置各种提供服务的途径，如rest api，socketio， Rasa X， facebook等等，这里不需要动；\nendpoints.yml：配置各种外接功能性url，如tracker_store等，在这里配置action server可能会用到action_endpoint的url来寻找action server；\ntests文件夹：不管。\n项目功能简介 查天气：通过和风天气api，传入时间地点获得天气信息；\n查歌曲：使用这里给出的网易云数据集，获取前200歌单信息，导入neo4j进行查询。\n","permalink":"https://boringleric.github.io/posts/tech/rasa3_deployment/","summary":"Rasa3 Demo部署简介，代码已挂载到Github。","title":"Rasa3 Demo部署样例"},{"content":"上一部分我们了解了Rasa对话机器人的大致框架，文件结构，以及数据流程，这次我们来学习Rasa的一个核心模块：NLU模块，它所负责的是将用户输入的文本进行拆解分词、实体抽取，并将这些识别的结果转为文本特征，送入意图识别器，得到用户输入文本的意图，并将该意图反馈给Rasa Core模块，同时也负责选择机器人对于用户问题的最优回复。\n在学习NLU模块之前，有一个小小的问题需要澄清一下，在Rasa 2.x版本中，一切流程都是以pipeline流水线模式执行，，但是Rasa 3.x版本对于结构进行了较大的调整，以图结构代替了传统的流水线结构，使得结构更加灵活，所以接下来将会对各个部分进行介绍。\nNLU框架 Rasa3.2.x 的NLU的源码结构图如下图所示：\n可以看出，NLU模块提供的主要功能有：\n分词（tokenizers）：Rasa官方支持的分词器有WhitespaceTokenizer（只支持英文，空格分词）、JiebaTokenizer（支持中文，结巴分词）、MitieTokenizer（基于C++开发，中文需要训练）、SpacyTokenizer（基于Cython的高速NLP库，支持中文模型），除此之外，也可以自定义分词器，在最后一部分的实践过程中将会实现基于Bert tokenizer的中文分词方法； 文本特征化（featurizers）：在特征化部分，Rasa给出了两类特征，一类是稀疏特征，一类是稠密特征。稀疏特征一般是基于统计的ngram特征、count_vectors特征（基于 scikit-learn）等，可能含有大量的0在其中，稠密特征是语言模型直接输出的文本特征，官方支持的语言模型有前面提到的Mitie、Spacy，还有基于transformers的一系列语言模型，如Bert等； 意图识别（classifiers）：将用户输入的信息归类到之前在domain.yml中列出的意图，以便机器人进行后续处理。有人可能会问，如果用户提问的不在之前所列的意图之中会怎么办，这里我们可以配置一个置信区间，当所有意图的置信度均达不到阈值，会有一个兜底策略（Fallback）回复用户，这个方法我们也会在最后部分的实践过程中给出展示。官方支持的分类器有Mitie、基于scikit-learn的逻辑回归、关键词分类器、自主研发可同时提取实体和做分类的DIET分类器以及用于兜底策略的兜底分类器； 实体抽取（extractors）：抽取用户输入信息中所具有的人名地名等实体，以供意图识别使用。官方支持的有Mitie、Spacy、CRF、Duckling、正则以及上面提到过的DIET分类器。附加：EntitySynonymMapper（实体同义词映射），这个模块会根据之前定义的同义词自动将识别的实体修正为同一个词汇，简化复杂度； 回复选择（selectors）：回复选择器的结构和前面提到的DIET结构一致，但它的目的则是检索意图对应的答案，选择给用户最合适的回复。 我们可以看出，上述几个功能中，使用最频繁的可以说是DIET这个模型了，接下来将会对这个模型进行解释，至于其他的NLP处理工具如Mitie、Spacy，感兴趣的可以去对应官网查阅相关资料。下面，开始介绍DIET。\nDIET介绍 DIET，全名为Dual Intent and Entity Transformer，是Rasa团队提出的可以用于意图分类和实体识别的多任务框架，它可以在仅使用词和字符级ngram的稀疏特征情况下，取得较高的准确性，当然也可以结合预训练模型的embedding来进一步提升任务的准确性，由于不需要pretrained embedding，减少了大量参数，所以模型的训练速度也大大提高，在CPU也可以轻松的完成训练。\nDIET的论文中给出了整个模型的架构，如下图所示：\n模型看上去并不复杂，从Loss角度看，可以看到这个图里面包含了三类任务：Mask词预测（Mask Loss）、意图识别（Intent Loss）、实体识别（Entity Loss）。其中Mask任务受启发于Pretrained LM训练过程，其Mask算法也与Pretrained LM一致，这项任务可以帮助模型从文本中学习到更多一般的特征，而其他两项任务则是和下游任务相关的。另外需要注意的一点，这三个任务是可以自由配置的，如果只希望模型做分类任务，那么完全可以关闭Mask任务和意图识别，以加速训练。\n接下来，我们从下往上一层层看。首先，输入的每个token可以有其sparse特征或dense特征（经过Pretrained embedding），为了融合sparse特征，会有一层前馈神经网络将稀疏特征转为稠密特征，经过一层concat之后就可以将融合的特征送入Feed-forward层，这里有一个很有意思的trick，为了提高模型训练效率，Feed-forward层使用的Dense层并非传统意义的全连接，而是设置了高达80%dropout的RandomlyConnectedDense；同时，在特征输入到Transformer之间的所有FF层，其权重都是共享的，从而进一步加快了模型训练速度。\n随后，融合特征被送入Transformer层进行编码，这里可以自由指定所需的transformer层数，Transformer结构和传统transformer结构一致，只是其中的Dense层全部被替换为RandomlyConnectedDense层。\n最后，如果做NER任务，那么编码后的特征会被送入CRF层进行实体识别，而cls向量会被用去做意图识别，若做Mask任务，则Mask的embedding会被拿来做token的embedding相似度匹配。\n总结 到这里为止，Rasa的NLU模块的大致框架就介绍完毕了，我们来总结一下，这个模块的主要功能在于对用户信息的NLU处理，得到意图送入Rasa Core，算法重点则在于DIET模型的理解，至于转换格式的Emulator、和将模型存储的persistor，相对来说不重要，就不需要额外解释了，下一步我们将一起学习Rasa Core在接收到NLU处理的内容后，对内容进行处理的过程。\n","permalink":"https://boringleric.github.io/posts/tech/rasa3_nlu/","summary":"Rasa3 NLU模块简介，以及核心DIET算法简介。","title":"Rasa3 NLU部分简介"},{"content":"在前面几节我们一起学习了Rasa的基本框架，对于Rasa机器人的组成部分、工作流程还有部分模块的工作原理有了一定的了解，接下来会对一些细节做补充，并介绍3.x版本的优化点。\n部分细节 Rasa源码的Telmetry是做什么的？Telemetry是Rasa用来匿名收集语言、策略、pipeline等数据，用来改善Rasa系统的信息收集系统，可以通过rasa telemetry disable停止其功能。 Rasa的Event Brokers是干啥用的？Event Broker主要用来跨域发布事件，比如将触发的event发送到其他服务进行消息通知或处理对应事件。 如何将Rasa引入自己的网站/如何在本机debug Rasa？Rasa提供了多样的channel，通过其提供的restful api或者Websocket可以很方便的将对话功能引入你的网站；本机调试简言之，python切换到bot文件夹，sys argv添加shell，执行rasa main即可。 Rasa是否包含ASR与TTS？可惜并没有。 Rasa Form的作用？如何填充？众所周知，在客服类型机器人中常常避免不了的一件事就是填表，无论是买票，订餐，还是售后，都需要有大量的待填充内容，以订机票为例，需要知道出发地、到达地、出发日期时间等等，这些信息在Rasa中会被组合成一个Form，而出发地之类的待填充内容则是From中的槽位（slot），前面我们提到的NLU里面的NER部分，在充分训练后，就可以自动在对话中提取所需的槽位并自动填充，而Policy预测Action会进入Form填充状态（active_loop），引导客户填充slot，直到所有slot填充完成或者被中断才会结束这个过程。 一般聊天机器人包含NLU和DM，Rasa的DM在哪里？DM一般来说指的是对话管理部分，也有狭义的认为是对话策略管理，从广义角度看来，整个Rasa Core模组就是DM，根据NLU的输入来决定下一步对话流程，单纯的对话策略管理模块（Policy）也是包含在Rasa Core的一部分。 最后，贴几个之前分析2.x数据流画的图，很简单粗暴，没啥道理，如果有兴趣可以去下载了看一下。\n3.x版本优化 在这里重点介绍两个功能：\n动态图功能：Rasa3.x更新了后台计算逻辑，将2.x的机器学习计算pipeline更新为图类型。在之前的版本中，NLU和Core的功能组件一直是各自独立的，虽然在端对端训练中，TEDPolicy也会使用到NLU的特征化模组，但是仍然不够灵活，如下图所示：\r因此在3.x版本中，将以上所有组件拆分，重构为有向无环图：\r可以看出，图中的每个节点依赖关系都非常明显，DIETClassifier对于Tokenizer并不关注，而RegexEntityExtractor则只关注Tokenizer的信息，从而更加便于并行化和高度定制化。而缓存功能也确保了在一个节点发生变化时，只需要其下游节点重新训练，节约了训练时间。 Markers功能：这个功能在3.x版本开发，主要为机器人功能的评估提供服务，通过在对话中标记兴趣点（Marker）触发条件，当满足条件时会触发Marker，以便于提供统计信息，对特定任务进行评估。Markers也是需要通过yml构造的，支持使用and/or/not等操作符来协助判断条件的触发类型。 实践发现的问题 Rasa是一个很完善的架构，但是并不意味着没有问题，在实际操作中，发现了以下几个不足：\n慢！过多的流程转移，导致一通对话响应时间较长，若加入语音的ASR和TTS，时延就不可想象了； 过多历史对话信息的存储严重拖慢了Policy推理的效率，这个应该可以通过设置存储对话轮次数来解决； 中文支持较欠缺； 过于臃肿。 若用于简单的客服场景还是比较推荐的，但是场景一旦复杂，涉及到语音转文本等内容，就比较不推荐了。当然，也不排除我学习的不够深入，还需要继续研究。\n总结 到此，Rasa的理论相关部分结束，下一部分将会进入实践阶段，以一个中文对话机器人案例，将所有的内容串接起来。\n","permalink":"https://boringleric.github.io/posts/tech/rasa3_others/","summary":"Rasa3 部分细节补充，包括Telmetry等，以及版本对比，部署发现的小问题等。","title":"Rasa3 部分细节"},{"content":"Rasa是一个经典的基于Python开发的开源对话机器人框架，经过多年完善，到目前已经发展到了3.x版本，其完备的框架可以确保只需少量训练数据，即可满足大部分客服机器人的需求。由于Rasa发展的实在迅速，本文撰写时，Rasa的版本为3.2.1，但是之前研究的主要是Rasa 2.8.x的结构，读者接触到的更高版本可能新增的特征本文不会涉及，在此表示歉意。\nRasa基本结构 Rasa2和3的基本架构很类似，均为下图所示的结构：\n可以看出，Rasa开源系统中主要包含这么几个部分：NLU、对话策略和Agent，而在源码中，对话策略和Agent被包含在了core模块中，而nlu则独自为一模块，两个模块分别可独立提供对话策略管理（core模块）和自然语言理解（NLU模块）功能，在后文我们将会对这两个部分做更详细的解读。\n除开上面的核心模块，在图中还可以看到有下面几个模块：\nAction Server：提供可扩展的Action及其对应能力给机器人调用； Tracker Store：在Rasa中，每一通对话都会被记录为一个tracker，顾名思义，这里就是存储tracker的地方，有内存存储（默认）、SQL存储、MongoDB存储、Redis存储、DynamoDB 存储以及自定义位置存储； Lock Store：为了确保对话的顺序进行，Rasa采用锁方式来锁定消息活动对话，而这些锁的信息也会被存在内存、Redis，或是自定义的位置； Filesystem(Model Storage)：为训练好的模型提供存储，可以存储在本地硬盘、服务器，甚至各类云服务器（Azure，Google等）上面。 Rasa训练文件结构 介绍了基本架构，下面来看一下Rasa的训练bot文件组成部分，一个完整的bot例子如下结构所示：\n其中各个部分的功能为：\nactions文件夹存放的是自定义的action及其对应的能力实现方式； data文件夹包含三个文件： nlu.yml：这个文件主要定义了机器人要识别的用户意图，以及对应意图的可能表述，如：意图“问好”，可能表述有你好、早上好、晚上好等等；此外，还可以在这里标注所需要识别的实体，添加同义词，添加识别意图和实体的正则表达式，添加实体速查表等功能； rules.yml：这个文件主要为识别到的意图提供强约束的机器人反馈，主要应用于短对话，并且只能根据已有的各种意图做出反应； stories.yml：这个文件主要存放训练机器人策略跳转的数据，可以为机器人提供未知对话情况下的反应能力； model文件夹主要存储训练好的模型； tests文件夹主要存储测试用例test_stories.yml； config.yml：为Rasa模型提供整体配置，包括使用语言、NLU处理流水线、对话策略等内容； domain.yml：这个文件定义了所有的意图、实体、槽位、机器人回复、需要填充的表单、自定义的action和机器人部分配置； endpoints.yml：这个文件可以配置机器人需要的第三方自定义功能，如nlg服务器的url等； credentials.yml：这个文件可以配置机器人要调用的第三方平台认证相关功能（没有使用过\u0026hellip;）。 Rasa信息处理流程 最后，我们来看一下Rasa对于消息的处理情况，当一条消息进入到Rasa机器人接口后，有下面的一个简单处理流程：\n首先，由NLU的Interpreter模块（Rasa2.x）/graph_runner（Rasa3.x）对输入文本进行处理，得到文本的一系列特征，如实体，意图等等；\n接下来，将识别到的一系列特征传入本轮对话的的Tracker，它会记录本轮对话的所有对话状态信息；\n记录完毕之后，Policy模块将会对Tracker的信息进行处理，并预测出要执行的Action；\n最后，执行Action，输出结果，更新Tracker。至此，一个粗略的Rasa信息处理流程结束。\n在接下来的部分里面，我们将会依次分析Rasa各模块的细节及使用到的模型和算法，最后，将会以一个中文对话机器人项目来结束这次的学习。\n","permalink":"https://boringleric.github.io/posts/tech/rasa3_structure/","summary":"Rasa3 基本框架简介。","title":"Rasa3 框架简介"},{"content":"前些日子参加了天池的[Text-to-SQL挑战赛](2022 WAIC 黑客松蚂蚁财富赛道：AntSQL大规模金融语义解析中文Text-to-SQL挑战赛-天池大赛-阿里云天池 (aliyun.com))，侥幸拿到了第七名，为什么这么说呢，因为纯粹是躺在巨人的肩膀上的。\n本来打算摸鱼玩玩，没想到后面玩上头了，对于Text2Sql问题，目前有两种解决思路，一种就是分开提取各种槽位，各种关系之类的最后再合并到一起；另一种就是生成式模型直接上。目前常见的两个数据集是Wikisql和Spider，懒狗自然用开源，所以选了[HydraNet](lyuqin/HydraNet-WikiSQL: Code and trained model for Hybrid ranking network for text-to-SQL on WikiSQL (github.com))作为基本模型结构。\n由于数据集是金融领域的，所以选择了finbert作为embedding。一开始我还是想改一下模型结构，后面发现改来改去效果也没有见得提升多少，反而因为加对抗加层数导致训练时间过长，对于上班狗而且就一个2080ti可以玩，这个时间确实有点紧张，做了一些traderoff决定：干脆不动模型结构，只在数据上面下功夫，毕竟garbage in，garbage out。\n这个竞赛的数据很奇怪，训练集基本上是各类模板生成的，也就是所谓的专家系统，而测试集则有一部分专家系统数据，还有一部分线上真实数据，因此造成了分布不一致问题，其实还是很影响结果的。所以我先想办法抽取模板，把模板词汇过滤出来，这样将训练集数据从8万简化到了4万左右，训练时间也提高很多，至少一晚上可以训练2个epoch。随后，对测试集出现的span错误情况，进行数据增强，使用nlpcda将表内信息提取重构，增加了8000条左右训练数据。\n最后，就是预处理和后处理。为什么要着重强调这两个，因为这俩真的很重要！训练数据里面太多的标准化数据需要映射为非标数据，而测试集里面太多的错字和非标信息需要转换。后处理则是将识别到的非标数据再反向映射到标准数据上面以便提升分数。\n说来说去我还是觉得这个名次是躺来的，真的没啥学术参考价值，要看参考价值还不如看看Wikisql和Spider排行榜上面的论文，最多就是警示算法狗们：数据分布一致很重要！\n最后的最后还是要diss DataFontain平台，玩不起别玩！\n","permalink":"https://boringleric.github.io/posts/tech/tianchi_text2sql/","summary":"躺赢的第三名，简单粗暴的处理逻辑。","title":"Text2Sql竞赛简单总结"},{"content":"HMM-CRF是做序列标注任务的标准算法之一。\n预备知识 EM算法 期望最大化算法（Expectation-maximization algorithm，EM）是在概率模型中迭代求解参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐性变量。主要分为两个步骤：\nE步：计算期望值，利用对隐藏变量的现有估计值，计算其最大似然估计值； M步：期望最大化，最大化在E步上求得的最大似然值来计算参数的值。 M步上找到的参数估计值被用于下一个E步计算中，迭代更新隐含数据和模型分布参数，直到收敛，即得到所需模型参数。\n要解决的问题 若要从样本观察数据中，找出样本的模型参数，最常用的方法就是极大化模型分布的对数似然函数。但若存在不可观察的隐含数据，因而无法直接用极大化对数似然函数得到模型分布的参数。这就是EM算法可以派上用场的地方了。\nEM算法解决问题的思路是使用启发式的迭代方法，既然无法直接求出模型分布参数，那么先猜想隐含数据（E步），接着基于观察数据和猜测的隐含数据一起来极大化对数似然，求解模型参数（M步)。由于隐藏数据是猜测的，所以此时得到的模型参数距离想要的结果还有差距，不过只要不断的迭代下去，直到模型分布参数基本无变化，就可以找到合适的模型参数。\n收敛性 1） EM算法能保证收敛吗？2） EM算法如果收敛，那么能保证收敛到全局最大值吗？\n生成式模型与判别式模型 在监督学习下，模型可以分为判别式模型与生成式模型。下图可见，NB在序列建模下拓展到了HMM；LR在序列建模下拓展到了CRF：\n判别式模型 判别式模型从数据的标签和提供的特征直接学习建模，通过函数映射等学习，最后得到比较明显可以切分标签的边界，如线性LR、线性SVM。 更准确地说，判别模型是直接对$P(Y|X)$建模，也就是直接根据X特征来对Y建模训练，确定$P(Y|X)$模型的参数。\n判别式模型特征有：\n对 $P(Y|X)$ 建模; 对所有的样本只构建一个模型，确认总体判别边界； 观测到输入什么特征，就预测最可能的标签。 优点：对数据量要求没生成式的严格，速度也会快，小数据量下准确率也会好些。\n生成式模型 生成式模型从训练样本数据中学习所有的数据分布情况，最终确定一个分布来作为所有的输入数据的分布，也就是所有的特征$x_i$和所有标签$y_i$的联合分布$P(X,Y)$ ，当进入推理过程时，通过条件概率公式$P(Y|X) = \\frac{P(X,Y)}{P(X)}$，可以推断新样本的类别。具体说，训练阶段是只对$P(X,Y)$建模，需要确定维护这个联合概率分布的所有的信息参数，在推理阶段再对新的样本计算$P(Y|X)$，获得对应标签。\n以朴素贝叶斯为例，学习阶段建模：$P(X,Y)=P(X|Y)P(Y)$，推理阶段$P(Y|X) = \\frac{P(X,Y)}{P(X)}$。\n生成式模型特征有：\n对$P(X,Y)$建模； 对于分类问题，要对每个标签都需要建模，最终选择最优概率的标签为结果，所以没有判别边界； 中间生成联合分布，并可生成采样数据。 生成式模型的优点在于，所包含的信息非常齐全，不仅可以用来输入label，还可以干其他的事情。生成式模型关注结果是如何产生的。但是生成式模型需要非常充足的数据量以保证采样到了数据本来的面目，所以速度相比之下，慢。\n马尔可夫假设 齐次马尔可夫假设：马尔可夫链 $x_1,⋯,x_n$里的$x_i$ 元素总是只受前一个元素$x_{i−1}$的影响，相当于2-gram。\n马尔可夫过程：在一个过程中，每个状态的转移只依赖于前n个状态，并且只是个n阶的模型。\n马尔可夫性：a. 成对，b. 局部，c. 全局。保证或者判断概率图是否为概率无向图的条件。\n有向图与无向图 以一个经典的图模型衍生图为例：\n上图可以看到，贝叶斯网络都是有向的，马尔科夫网络无向。所以，贝叶斯网络适合为有单向依赖的数据建模，马尔科夫网络适合实体之间互相依赖的建模，其核心差异表现在如何表示$Y = ({y_1},\u0026hellip;,{y_n})$这个的联合概率。\n1. 有向图\n有向图的每个节点对应一个随机变量，通过条件概率$P(x_i│Parents(x_i))$刻画父节点对$x_i$的影响，图中无回路，求联合概率有下列公式：$P({x_1},\u0026hellip;,{x_n}) = \\prod\\limits_{i = 0} P({x_i}|\\pi (x_i))$ 。\n对于下面的有向图：\n联合概率可以表示为：$P({x_1},\u0026hellip;,{x_n}) = P({x_1})P({x_2}|{x_1})P({x_3}|{x_2})P({x_4}|{x_2})P({x_5}|{x_3},{x_4})$。\n2. 无向图\n无向图的每个节点也对应一个随机变量，每条边表示随机变量之间的依赖关系，联合概率分布满足局部马尔科夫性。一般的图结构如下所示：\n定义最大团的概念：无向图中任何两个结点均有边连接的结点子集称为团。如果无向图C是一个团，并且不能再加进任何一个结点使其成为更大的团，则称C为最大团。如上图$X_1,X_3,X_4$和$X_2,X_3,X_4$分别是两个最大团，而$X_1,X_2,X_3,X_4$并不是最大团，因为$X_1$和$X_2$没有连线。\n所以对于一个无向图，可以将联合概率写为若干个最大团联合概率的乘积，有：$P(Y) = \\frac{1}{Z(x)}\\prod _{c} \\psi _{c} (Y_c)$，其中$Z(x) = \\sum\\nolimits_Y {\\prod\\nolimits_c {{\\psi _c}(Y_c)}}$，归一化处理；${\\psi _c}({Y_c})$表示最大团C上随机变量的联合概率，也称作势函数。\n上图可以表示为：$P(Y) = \\frac{1}{{Z(x)}}({\\psi _1}(X_1,X_3,X_4) \\cdot {\\psi _2}(X_2,X_3,X_4))$。\nHMM算法 常规的序列数据一般用马尔科夫模型就可以表达，但实际使用HMM的场景是每个节点$X_i$下还附带着另一个节点$Y_i$ ，也就是隐含马尔科夫模型，那么除了可见的节点，还要将隐含状态节点一起建模。所以对于HMM，将$X_i$、$Y_i$ 换成$i_i$、$o_i$ ，名称变为状态节点和观测节点，其中状态节点就是隐状态，如下图所示：\nHMM属于有向图，是典型的生成式模型，要从训练数据中学到数据的各种分布，正是HMM的5要素，其中有3个就是整个数据的不同角度的概率分布：\n$N$：隐藏状态集$N = {{q_1},\u0026hellip;,{q_N}}$，包含所有隐藏节点的状态； $M$：观测集$M = { {v_1},\u0026hellip;,{v_M}}$，包含所有观测节点的状态； $A$：状态转移概率矩阵，$A = [a_{ij}]{N \\times N}$，N是隐状态数量，$a{ij} = P(i_{t + 1}|i_t)$，$i_t$就是第t个隐状态节点，也就是遵循齐次马尔科夫链假设； $B$：观测概率矩阵，$B = [b_{ij}]{N \\times M}$，N是隐状态数量，M是观测状态数量，$b{ij} = P(o_t|i_t)$，$o_t$是第t个观测节点，$i_t$就是第t个隐状态节点，意味着o对i有依赖性，也就是观测独立性假设； $\\pi$：初始状态概率分布，为第一个隐状态$i_t$分配概率。 在学习过程中，HMM会确定以上5要素，在推理过程：隐状态节点$i_t$是不能直接观测到的数据节点，$o_t$才是能观测到的节点，并且注意箭头的指向表示了依赖生成条件关系，$i_t$在$A$的指导下生成下一个隐状态节点$i_{t+1}$，并且$i_t$在$B$的指导下生成依赖于该$i_t$的观测节点$o_t$。\n以序列标注任务，实体识别为例，所有文本里的token构成的列表是观测集M ，因为token序列在推理阶段是可见的；标注集BIEOS构成隐藏状态集N，这是模型的预测任务；至于A、B、$\\pi$的概率分布信息是在学习过程中确定的参数。\n模型参数学习过程 HMM学习训练的过程，就是给定观测序列$O={o_1,o_2,\u0026hellip;o_T}$，找出数据的分布情况，也就是模型参数A，B和$\\pi$的确定，使观测序列的条件概率最大。学习算法主要有以下两种：\n在有隐状态序列的情况下，使用极大似然估计； 在没有隐状态序列的情况下，使用Baum-Welch(前向后向)算法。 1. 极大似然估计\n一般做NLP的序列标注等任务，在训练阶段肯定是有隐状态序列的，步骤很简单：\n计算A：$a_{ij} = \\frac{A_{ij}}{\\sum_{j = 1}^N A_{ij}}$； 计算B：$b_j = \\frac{{B_{jk}}}{\\sum\\nolimits_{k = 1}^M {B_{jk}}}$； 最后估计$\\pi$。 2. Baum-Welch(前向后向)\nBaum-Welch是一个EM的过程，步骤如下：\n随机初始化所有的A，B，$\\pi$； 对于每个样本，使用前向后向算法计算联合概率分布； 更新模型参数，若参数没有收敛，返回2继续迭代。 预测（解码）过程 对于预测过程，就是给定模型参数A，B，$\\pi$和观测序列$O={o_1,o_2,\u0026hellip;o_T}$，求给定观测序列条件下，最可能出现的对应的隐状态序列，也就是已知了$P(Q,O)$，现在要求出$P(Q|O)$，即$Q_{max} = \\arg {\\max _{allQ}}\\frac{P(Q,O)}{P(O)}$。\n使用的算法是维特比算法，在HMM中维特比算法定义了两个局部状态用于递推。第一个局部状态是在时刻t隐藏状态为i所有可能的状态转移路径$i_1,i_2,\u0026hellip;,i_t$中的概率最大值，第二个局部状态由第一个局部状态递推得到。\n评估观测序列概率 评估观测序列概率就是给定模型参数$\\lambda=(A,B,\\pi)$和观测序列$O={o_1,o_2,\u0026hellip;o_T}$，求给定模型下观测序列条件出现的概率$P(O|\\lambda)$。\n主要的计算有三类算法：1. 直接计算法（穷举搜索）；2. 前向算法（DP）；3. 后向算法（DP）\nMEMM MEMM，即最大熵马尔科夫模型，属于判别式模型，直接为了确定边界使用后验概率建模。\n在HMM中，观测节点$o_i$依赖隐藏状态节点$i_i$。但在更多的实际场景下，观测序列是需要很多的特征来刻画的，而MEMM模型则允许直接定义特征，学习条件概率$P(i_t|{i_{t - 1}},o_t)(t = 1,\u0026hellip;,n)$，这个概率通过最大熵分类器建模，得到： $$ P(i|i\u0026rsquo;,o) = \\frac{1}{Z(o,i\u0026rsquo;)}\\exp (\\sum\\nolimits_a {\\lambda _a{f_a}(o,i)} ) $$ 其中$Z(o,i\u0026rsquo;)$表示归一化，${f_a(o,i)}$是特征函数，需要手动定义，$\\lambda$是特征函数的权重，需要从训练中学习得到。\nMEMM需要注意：\n与HMM的$o_t$依赖$i_t$不一样，MEMM当前隐藏状态$i_t$依赖当前时刻的观测节点$o_t$和上一时刻的隐藏节点$i_{t−1}$； 图的箭头方向，是由MEMM的公式决定的，公式是手动定义的。 可以得到流程图：\n总结流程：\n定义特征函数${f_a(o,i)}$； 训练模型，确定参数； 序列标注或者序列求概率。 参数学习训练过程 MEMM参数同样需要通过训练数据学习得到，有极大似然估计、梯度下降等方法。\n序列标注过程 与HMM一致，用学习好的MEMM模型，在观测序列$o_1,⋯,o_t$上找出一条概率最大的隐状态序列$i_1,⋯,i_t$。路径求解过程也是采用维特比算法。\n评估观测序列概率 与HMM一致，分别为每一批数据训练构建特定的MEMM，然后根据序列在每个MEMM模型的不同得分概率，选择最高分数的模型。\n标注偏置问题 MEMM存在序列标注过程中的标注偏置问题。\n用维特比算法解码MEMM，状态1倾向于转换到状态2，同时状态2倾向于保留在状态2：\n$P(1 \\to 1 \\to 1 \\to 1) = 0.4 \\times 0.45 \\times 0.5 = 0.09$ $P(2 \\to 2 \\to 2 \\to 2) = 0.2 \\times 0.3 \\times 0.3 = 0.018$ $P(1 \\to 2 \\to 1 \\to 2) = 0.6 \\times 0.2 \\times 0.5 = 0.06$ $P(1 \\to 1 \\to 2 \\to 2) = 0.4 \\times 0.55 \\times 0.3 = 0.066$ 但得到的最优的状态转换路径是$1 \\to 1 \\to 1 \\to 1$，因为状态2可以转换的状态比状态1要多，从而使转移概率降低。所以MEMM倾向于选择拥有更少转移的状态，以提高整体的后验概率。\n由于MEMM的归一化过程在指数内部，所以属于局部归一化，而维特比求解过程中，使用的状态转移公式无法正确低轨道全局最优解。\nCRF 一般情况下CRF专指线性链CRF（Linear chain CRF）：\n无向图的联合概率分布可以分解为： $$ P(Y|X) = \\frac{1}{Z(x)}\\prod\\nolimits_c e^{\\sum\\nolimits_k {\\lambda k}f_k (c,y|c,x)} = \\frac{1}{Z(x)} e^{\\sum\\nolimits_c \\sum\\nolimits_k {\\lambda k}f_k (y_i,y{i - 1},x,i)} $$ 在线性链CRF中，每一个I-O对都是一个最大团，且满足$P({I_i}|O,{I_1},\u0026hellip;,{I_n}) = P({I_i}|O,{I{i - 1}},{I_{i + 1}})$。\n因此CRF建模公式为： $$ P(Y|X) = \\frac{1}{Z(x)} e^{\\sum\\nolimits_c \\sum\\nolimits_k \\lambda k f_k (y_i,y{i - 1},x,i)} = \\frac{1}{Z(O)} e^{\\sum\\nolimits_i^T \\sum\\nolimits_k^M \\lambda k f_k (O,I{i - 1},I_i,i)} $$ 其中下标i表示当前所在节点位置；k表示第几个特征函数，每个特征函数都有权重$\\lambda _k$，也就是每个团里面为每个token构造M个特征，建模时为每个特征函数加权求和；$Z(O)$表示归一化，形成概率值；\nCRF特征函数：$\\sum\\nolimits_i^T \\sum\\nolimits_k^M {\\lambda k}f_k(O,I{i - 1},I_i,i) $，为每个token打分，最后将所有分数进行log linear表示，求和后归一化就可以得到概率值。\n模型的工作流程以及三个任务同MEMM一致。\nLSTM+CRF LSTM和CRF都可以预测token的标签，但是预测机理是不同的。CRF是全局范围内统计归一化的条件状态转移概率矩阵，再预测出一条指定的sample的每个token的label；LSTM是依靠神经网络的超强非线性拟合能力，在训练时将特征通过高维空间的非线性变换，学习出一个模型，然后再使用softmax预测出每个token的标签。\nLSTM存在标注错误问题，如：\n输入：学习出一个模型 期望输出：学/B习/E 出/S 一/B个/E模/B型/E LSTM输出：学/E习/E 出/S 一/B个/B模/B型/E 因为softmax只做了局部的考虑，没有考虑前后词之间标签的关系，上述错误在CRF中是不存在的，因为CRF的特征函数就是为了观察学习各种特征，也就是限定窗口下的各种词之间的关系，如：开头是E，B后面接E，不会出现B。这个限定特征会使得CRF的预测结果不出现上述例子的错误。\n在LSTM+CRF中，CRF的特征分数直接来源于LSTM传上来的隐状态的值；而在经典CRF中，分数是统计来的。隐状态本身当做特征分数形成转移矩阵再让viterbi进行路径搜索，这就是CRF的意义。\n对比 HMM -\u0026gt; MEMM： HMM模型中存在两个假设：（1）观测值之间严格独立；（2）齐次马尔科夫假设，状态的转移过程中当前状态只与前一状态有关。但实际上序列标注问题不仅和单个词相关，而且和观察序列的长度，单词的上下文等相关。MEMM解决了HMM观测独立性假设的问题。因为HMM只限定在了观测与状态之间的依赖，而MEMM引入自定义特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖。 MEMM -\u0026gt; CRF：（1）CRF不仅解决了HMM输出独立性假设的问题，还解决了MEMM的标注偏置问题，MEMM容易陷入局部最优是因为只在局部做归一化，而CRF统计了全局概率，在做归一化时考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题，使得序列标注的解码变得最优解。（2）HMM、MEMM属于有向图，所以考虑了x与y的影响，但没把x当做整体考虑进去（这点问题应该只有HMM），CRF打破了HMM 的齐次马尔科夫假设，属于无向图，没有这种依赖性，克服此问题。 参考 隐马尔科夫模型HMM 如何用简单易懂的例子解释条件随机场（CRF）模型？ ","permalink":"https://boringleric.github.io/posts/tech/hmm_crf/","summary":"HMM-CRF作为传统序列标注任务算法，还是有必要看一看，虽然看了就忘\u0026hellip;","title":"HMM到CRF的简单了解"},{"content":"昨天想着玩一下GNN，然后去下载PYG，在跑一个demo的时候，报了一个错误：\nversion `GLIBC_2.27\u0026rsquo; not found\n查了一下，发现是系统库版本太低，然后有人就说可以自己编译一下，很好，动手下载GLIBC_2.27源码，编译执行，又报错，这次是莫名其妙的“segment fault”，怎么办呢，继续查，发现可能是gcc版本太低，make版本太低的原因，所以还有啥说的呢，继续更新吧。\n就在这时！突然啥指令都执行不了啦！无论sudo，ln，ls\u0026hellip;全都是报下面的错误：\nrelocation error: /lib64/libpthread.so.0: symbol __libc_dl_error_tsd ver\u0026hellip;.\n仔细一看发现，好嘛虽然segment fault了，但是仍然生成了2.27.so，然后不知道咋回事还把libc.so.6给软链接过去了，不兼容导致一切都无法执行，想着删除，再软连接回去，无能为力，只好重装。\n重装就没啥可说的了，干干净净的\u0026hellip;\u0026hellip;\n重装完以后发现，CUDA装不上了？？？？\n首先，没有perl：好说，yum -y install gcc gcc-c++ perl make kernel-headers kernel-devel； 装完了，这次呢，“The driver installation is unable to locate the kernel source. ”？？？：查了下，内核版本不兼容，好，升级内核，yum install kernel，reboot -h now，重启一看，uname -r和kernel-devel一致，这下可以了吧； 很好，这个错误没有了，但是还是安装失败，这次换了这个错误“drm_dev_free虚函数blabla”，继续查：有人说安装dkms，好，先yum install -y epel-release，再yum install -y kernel-headers kernel-devel dkms，重启； 这次可以了，做了nvcc的link也可以执行nvcc -V，但是！为什么nvidia-smi显示的是“NVIDIA-SMI has failed because it couldn\u0026rsquo;t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.”？？？？明明刚装的，你就找不到了？？？？继续查：有人说用dkms装就行，很不幸这个装都装不上！又有人说要停用第三方驱动nouveau，但是我lsmod | grep nouveau连个鬼影都不见！最后，突然想到，我直接去Nvidia官网下载一个驱动试试，没想到还真行了！真见鬼！ 最后，千万不要手贱去自己编译什么GLIBC！还有，Conda的Tensorflow是真的奇葩，没有GPU库？还是得用pip去装！\n最后的最后，升级了Centos到了Debian，还是Debian最好用\u0026hellip;装驱动也方便！\n","permalink":"https://boringleric.github.io/posts/tech/glibc_system_error/","summary":"GLIBC千万不要随便升级啊！","title":"一次升级GLIBC把Centos玩崩的经历"}]